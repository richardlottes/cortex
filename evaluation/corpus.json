[
  {
    "id": 0,
    "name": "attention_is_all_you_need.txt",
    "content": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.com\nNoam Shazeer\u2217\nGoogle Brain\nnoam@google.com\nNiki Parmar\u2217\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.com\nAidan N. Gomez\u2217 \u2020\nUniversity of Toronto\naidan@cs.toronto.edu\n\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7 [cs.CL] 2 Aug 2023\n1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\nScaled Dot-Product Attention Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by \u221a\ndk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax(QKT\n\u221a\ndk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof \u221a\n1\ndk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4\n. To counteract this effect, we scale the dot products by \u221a\n1\ndk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k =\nPdk\ni=1 qiki, has mean 0 and variance dk.\n4\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)WO\nwhere headi = Attention(QWQ\ni\n, KW K\ni\n, V WV\ni\n)\nWhere the projections are parameter matrices W\nQ\ni \u2208 R\ndmodel\u00d7dk , W K\ni \u2208 R\ndmodel\u00d7dk , WV\ni \u2208 R\ndmodel\u00d7dv\nand WO \u2208 R\nhdv\u00d7dmodel\n.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndf f = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221a\ndmodel.\n5\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n\n2\n\u00b7 d) O(1) O(1)\nRecurrent O(n \u00b7 d\n2\n) O(n) O(n)\nConvolutional O(k \u00b7 n \u00b7 d\n2\n) O(1) O(logk(n))\nSelf-Attention (restricted) O(r \u00b7 n \u00b7 d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel)\nP E(pos,2i+1) = cos(pos/100002i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi\n, zi \u2208 R\nd\n, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k \u00b7 n \u00b7 d + n \u00b7 d\n2\n). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129\n. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d\n\u22120.5\nmodel \u00b7 min(step_num\u22120.5\n, step_num \u00b7 warmup_steps\u22121.5\n) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 \u00b7 1020\nGNMT + RL [38] 24.6 39.92 2.3 \u00b7 1019 1.4 \u00b7 1020\nConvS2S [9] 25.16 40.46 9.6 \u00b7 1018 1.5 \u00b7 1020\nMoE [32] 26.03 40.56 2.0 \u00b7 1019 1.2 \u00b7 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \u00b7 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 \u00b7 1020 1.1 \u00b7 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 \u00b7 1019 1.2 \u00b7 1021\nTransformer (base model) 27.3 38.1 3.3 \u00b7 1018\nTransformer (big) 28.4 41.8 2.3 \u00b7 1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5\n.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN dmodel dff h dk dv Pdrop \u03f5ls\ntrain PPL BLEU params\nsteps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434\u2013443. ACL, August 2013.\n12\nAttention Visualizations\nInput-Input Layer5 Itisinthis spirit that\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13\nInput-Input Layer5 The Law will never beperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5 The Law will never beperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\nInput-Input Layer5 The Law will never beperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5 The Law will never beperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15"
  },
  {
    "id": 1,
    "name": "coffee_brewing.txt",
    "content": "What Coffee-Brewing Method Makes the Best-Tasting Cup?\nJill Waldbieser\nupdated Sep 14, 2022\n\nSave\nfacebook\npinterest\nemail\n\nCopy URL\nPage URL\nhttps://www.thekitchn.com/best-coffee-brewing-method-22977756\ncomments\n78\nWe independently select these products\u2014if you buy from one of our links, we may earn a commission. All prices were accurate at the time of publishing.\nIt doesn\u2019t take much digging through the vast array of coffee-making apparatuses, either at your local kitchen store or online, to get overwhelmed. There are drip machines, pod-machines, French presses \u2014 even this odd-looking contraption.\n\nWhen selecting the best brewer for you, you\u2019ll want to consider a variety of factors: How fast is it? How much cleanup is required? How much coffee does your household consume? And of course, there\u2019s taste.\n\nFor Me, Great Coffee Tastes Like Italy\nSince I started drinking coffee in college, I\u2019ve cycled through practically every way there is to brew and enjoy the stuff. When convenience was a priority, I frequented vending machine dispensers and used K-cups. Eventually I got into higher-quality coffee, and wanted to make it at home. I bought a pourover (more on that, below) and started grinding my own beans.\n\nI thought I knew everything. Then I went to Italy. \n\nHow To Make Easy Pan-Fried Pork Chops\n00:31\n\n01:06\n\n\n\n\n\n\n\n\n\n\nItaly changed my ideas about what good coffee was, and about what it was worth doing to get that good cup. Even the stuff you buy at rest-stop gas stations there tasted infinitely superior to anything I\u2019d ever had stateside.\n\nWhen I got back, I was motivated to improve my home-coffee game. So I consulted with a couple coffee-making experts, Dan Pabst at Melitta and Giorgio Milos, master barista for Illy, to see where I could improve my brew.\n\nWhat Makes Great Coffee, According to Experts\nPabst and Milos broke it down to its simplest components for me. A brew\u2019s flavor derives from two primary things: There\u2019s where and how the beans are grown (the terroir, like wine) and how darkly or lightly it\u2019s roasted.\n\nWhen it comes to roasting, Milos takes the view that there is no right or wrong level, it comes down to personal taste. He compared the preference for lighter roasts versus darker ones to liking steak rare versus well-done.\n\n\nBut what about the brewing process itself? With so many methods, surely there were differences in the taste of the final cup that went beyond the bean. There, Pabst and Milos had opinions, but to really find out firsthand, I decided to stage a cage-match style cup-off. I chose five of the most popular home-brewing methods to compete against each other for my own personal single best cup of coffee. Best cup, in this case, being the one that tasted most like I had enjoyed in Italy.\n\nFive cups of coffee labeled Aeropress, Drip Machine, Pourover, Moka Pot, and French Press on a marble surface.\nCredit: Joe Lingeman\nHow I Tested Coffee Methods\nThere are a lot of considerations that go into choosing a coffee-making method, including quantity, economy, and convenience. But for this test I wanted to determine which method simply yields the single-best cup of coffee. And my basis for consideration was my platonic ideal: the rich, smooth, complex, just slightly bitter brew I found in Italy.\n\nIn order to remove as many extraneous variables as possible, I didn\u2019t give any consideration to whether a method makes a lot of coffee or a single cup, and I didn\u2019t pay much attention to how complicated or time-consuming a method might be. Taste was the goal, although I realize that a coffee method is often more about how many people you need to serve or how long you have.\n\nThen there was the choice of coffee beans and roast. Although my own preferences tend toward something a little more locally-sourced, I wanted to make this as even-handed as possible. I decided to use a high-quality, but widely available medium roast coffee: Starbucks Pike Place medium roast beans. If you decide to repeat my methods, you should most certainly use the beans (and the roast level) you prefer.\n\n\nI bought my beans whole and used a burr grinder (see our current favorite here) to grind each batch fresh, and to the recommended level of coarseness. (To find the correct grind size for each method, I consulted this guide from the home-brewing blog Home Grounds.) Both experts agreed that a burr grinder, which crushes the beans and results in an even-sized grind, is infinitely better than a blade grinder. Both also noted that pre-ground beans can work perfectly well for most methods (though some require a particular size grind), and will be perfectly fresh if used within a week or so.\n\nI used the amounts recommended by the manufacturer, unless otherwise specified, and tap water, heated with a gooseneck kettle, in all tests. All brews were judged black, without sugar or cream, also to reduce the variables.\n\nI also followed Pabst\u2019s advice to use paper filters for every method that needed them, not the reusable ones the eco-conscious cook in me loves. He explained that paper absorbs some of the oils from the beans, which can interfere with the taste. Armed with this knowledge, a bag of beans, and a kettle, I prepared to get my buzz on and determine the best way to brew once and for all. \n\nOXO Brew Conical Burr Coffee Grinder at Williams Sonoma\nOXO Brew Conical Burr Coffee Grinder\n$99.95\nWilliams Sonoma\nBuy Now\n\nSave\nSpoon adding ground coffee into a Cuisinart coffee maker.\nCredit: Joe Lingeman\nMethod: Drip Machine \nTime: 8 minutes\nFlavor Rating: 5/10\nThere are some high-tech coffeemakers out there, but the one I used was a standard- issue drip machine, the kind you\u2019re likely to encounter in waiting rooms and other public places. Its most forward-thinking feature is probably a charcoal filter for the water. Adjustable settings allow you to pick your desired java strength (regular or bold) and temperature. After consulting the chart, I added three tablespoons of medium-grind coffee to the refillable filter basket, chose the regular strength and medium heat, and commenced brewing.\n\n\nDrip Machine Results: I wasn\u2019t expecting much, but I was secretly hoping to be pleasantly surprised so I could shock the coffee snobs. Alas, despite a deep appreciation for programmable machines that let you have a hot cuppa waiting for you, the flavor really was lacking. The coffee was much thinner and more translucent than all my other tests, and the taste was equally watered down. \n\nGlass Carafe Handle Programmable Coffeemaker at Williams Sonoma\nGlass Carafe Handle Programmable Coffeemaker\n$99.95\nWilliams Sonoma\nBuy Now\n\nSave\nPouring hot water into a French press with coffee grounds.\nCredit: Joe Lingeman\nMethod: French Press\nTime: 5 minutes\nFlavor Rating: 7/10\nI was on a daily French press kick for a while so the process was familiar. I added 42 grams of coffee, ground to the French press setting on my grinder, to the carafe. Then, I poured in water that was just below boiling, around 175\u00b0F. I let it steep for four minutes, using a timer. While steeping, I swirled the grounds in the water, which is supposed to give better extraction than stirring. Then I depressed the plunger to trap the grounds at the bottom, and poured my coffee. The whole process seems fussy, but honestly waiting is the toughest part.\n\nFrench Press Results: Given that the grounds come into the most prolonged contact with the water in this method, I expected it would be the boldest tasting. It wasn\u2019t, but it wasn\u2019t bad. The color and aroma were both lighter than pour over, and I could see the oils from the coffee beans on top, left behind because there was no paper filter to trap them. The taste was enjoyable, smoother and noticeably less bitter than drip \u2014 but it also lacked a certain depth and punch.\n\nBodum Chambord French Press Coffee Maker at Sur La Table\nBodum Chambord French Press Coffee Maker\n$49.95\nSur La Table\nBuy Now\n\nSave\nGround coffee being scooped into a moka pot on a kitchen counter.\nCredit: Joe Lingeman\nMethod: Moka Pot\nTime: 10 minutes\nFlavor Rating: 8/10\nThe Bialetti Moka Pot, that iconic little metal two-story hexagonal stovetop coffee maker, uses pressure extraction \u2014 similar to espresso. This is the classic Italian method, and the one I\u2019ve been using to make my daily brew since I went to Italy last year, so I thought I had this method down pat. But on talking with Pabst and Milos, I discovered that I could be doing a few things differently.\n\n\nThe base holds water, which I learned should be just below boiling before you put it on the stove. When the water boils, it generates steam, which forces the water up through the basket of grounds resting on top of it and into the top chamber. I had also been told not to pack the grounds too tightly, because it can block the water, and cause too much pressure to build up. And I discovered that you can get a better cup of coffee if you brew with the lid open; then, as soon as the coffee starts flowing out of the spout, you remove it from the heat, close the lid, and wrap the base with a chilled towel or run it under cool water to stop the extraction process. This prevents the coffee from being too bitter. \n\nMoka Pot Results: I still love this method, maybe even a little more now that I\u2019m using this new method. It\u2019s kind of Zen to set it up, the cleanup is easy, and the sputtering of coffee brewing will forever remind me of Italy. It makes a darn good cup, too: The coffee is steamy, thick, and dark brown with a strong, pure aroma. A sip delivers that classic robust, slightly bitter flavor to my tastebuds. The main problem I\u2019ve found is that the metal filter cup leaves some sediment in my cup, but I consider that a small price to pay. Ironically, however, two methods seemed to yield a cup that was even better than the classic Italian way: The pourover and the Aeropress, below.\n\nBialetti Moka Expresso Espresso Maker at Williams Sonoma\nBialetti Moka Expresso Espresso Maker\n$39.95\nWilliams Sonoma\nBuy Now\n\nSave\nPouring hot water over coffee grounds in a paper filter on a white ceramic dripper.\nCredit: Joe Lingeman\nMethod: Pourover\nTime: 5 minutes\nFlavor Rating: 9/10\nI\u2019ve been a pourover fan for a long time, mainly because cleanup is so easy. I suffered eco-guilt at the thought of paper filters, though, and switched to reusable ones a while back. This test convinced me to switch back, and compost the filters I use instead.\n\nFollowing these directions, I placed a paper filter in my pour over and poured right-off-the-boil water into wet the filter to remove any paper taste. Then I added the grounds, 21 grams of a medium-fine grind, the consistency of table salt. (This was actually much finer than what I had typically used.) The first pour of the pourover process is supposed to \u201cbloom\u201d the grounds, or wet them and allow them to release their flavors. I stirred the grounds at this point, something I had also never done before. Then I added more water in a longer, swirling pour, followed by smaller quick hits until the mug was full. \n\n\nPourover Results: If this process seems a bit extra for your basic morning cup of coffee, I\u2019m with you. But it\u2019s not actually difficult. It\u2019s slow, making only one cup at a time, but with practice it almost becomes second nature \u2014 and the results are unbelievable. The pourover yields a dark, thick liquid with a rich, robust taste. I never noticed the thin sheen of oils on coffee before, but using a paper filter definitely eliminates them. \n\nKalita Wave 185 Dripper at Amazon\nKalita Wave 185 Dripper\n$42.48\nAmazon\nBuy Now\n\nSave\nHands pressing an AeroPress coffee maker over a glass cup on a countertop.\nCredit: Joe Lingeman\nMethod: AeroPress\nTime: 2 minutes\nFlavor Rating: 10/10\nAbout: I\u2019ll admit that, after my pourover test, I was convinced it couldn\u2019t get better. But this device, designed by a Stanford University professor, has a cult-like following among certain coffee enthusiasts.\n\nIt certainly doesn\u2019t look fancy. It looks like a plastic bike pump and works somewhat like a French press. After adding a paper filter to the bottom of the chamber, you put the coffee (ground midway between espresso and a drip coffeemaker) and hot water in, stir, and then use the plunger to force the water through the grounds into your cup. While a French press, requires a four minute steep, this method only sits 10 seconds before plunging (though it\u2019s recommended that you wet the filter first). \n\nAeropress Results: The AeroPress, like a pourover or Moka pot, makes only one cup at a time, but it\u2019s one superb cup. It\u2019s dark, smooth, and rich with very little bitterness. (Because brewing happens so quickly, there\u2019s no time for any undesirable elements to be extracted.) It\u2019s fast, cleanup is easy, and you can pretty much use it anywhere \u2014which is why it\u2019s a favorite among camping enthusiasts. The only drawback I can see is the need to buy specialty filters. For coffee this good, it\u2019s worth it. \n\n\nAeropress Results: The AeroPress, like a pourover or Moka pot, makes only one cup at a time, but it\u2019s one superb cup. It\u2019s dark, smooth, and rich with very little bitterness. (Because brewing happens so quickly, there\u2019s no time for any undesirable elements to be extracted.) It\u2019s fast, cleanup is easy, and you can pretty much use it anywhere \u2014which is why it\u2019s a favorite among camping enthusiasts. The only drawback I can see is the need to buy specialty filters. For coffee this good, it\u2019s worth it. \n\nAeroPress Coffee and Espresso Maker at Williams Sonoma\nAeroPress Coffee and Espresso Maker\n$39.95\nWilliams Sonoma\nBuy Now\n\nSave\nFinal Thoughts\nLearning how to better my brew was an eye-opening experience, and not just because of the combined caffeine of six-plus cups of coffee. It\u2019s good to know that Italian caf\u00e9-level quality isn\u2019t out of reach for a home brewer, if that\u2019s what you\u2019re going for, and that there are choices that fit every lifestyle and need, from the six-cup-a-day family to the guy or gal who just wants a halfway-decent grab-and-go option. Like me, your coffee needs will probably change over your lifetime, and now you know how to get the maximum flavor out of any brewing method. And once you have good coffee, everything else is pretty much cake.\n\nYour turn: What\u2019s your favorite method of making coffee, and why? Tell us in the comments, below."
  },
  {
    "id": 2,
    "name": "color_theory.txt",
    "content": "Basic Color Theory\nColor theory encompasses a multitude of definitions, concepts and design applications - enough to fill several encyclopedias. However, there are three basic categories of color theory that are logical and useful : The color wheel, color harmony, and the context of how colors are used.\n\nColor theories create a logical structure for color. For example, if we have an assortment of fruits and vegetables, we can organize them by color and place them on a circle that shows the colors in relation to each other.\n\nfruit organized by color\n\nThe Color Wheel\nA color circle, based on red, yellow and blue, is traditional in the field of art. Sir Isaac Newton developed the first circular diagram of colors in 1666. Since then, scientists and artists have studied and designed numerous variations of this concept. Differences of opinion about the validity of one format over another continue to provoke debate. In reality, any color circle or color wheel which presents a logically arranged sequence of pure hues has merit.\n\nThree color wheels - Harris, Today, Goethe \n\n\nThere are also definitions (or categories) of colors based on the color wheel. We begin with a 3-part color wheel.\n\n \n\nPrimary Secondary Tertiary Colors\n\nPrimary Colors: Red, yellow and blue\nIn traditional color theory (used in paint and pigments), primary colors are the 3 pigment colors that cannot be mixed or formed by any combination of other colors. All other colors are derived from these 3 hues. \n\nSecondary Colors: Green, orange and purple\nThese are the colors formed by mixing the primary colors.\n\nTertiary Colors: Yellow-orange, red-orange, red-purple, blue-purple, blue-green & yellow-green\nThese are the colors formed by mixing a primary and a secondary color. That's why the hue is a two word name, such as blue-green, red-violet, and yellow-orange.\n\n Color Harmony\nHarmony can be defined as a pleasing arrangement of parts, whether it be music, poetry, color, or even an ice cream sundae.\n\nIn visual experiences, harmony is something that is pleasing to the eye. It engages the viewer and it creates an inner sense of order, a balance in the visual experience. When something is not harmonious, it's either boring or chaotic. At one extreme is a visual experience that is so bland that the viewer is not engaged. The human brain will reject under-stimulating information. At the other extreme is a visual experience that is so overdone, so chaotic that the viewer can't stand to look at it. The human brain rejects what it cannot organize, what it cannot understand. The visual task requires that we present a logical structure. Color harmony delivers visual interest and a sense of order.\n\nIn summary, extreme unity leads to under-stimulation, extreme complexity leads to over-stimulation. Harmony is a dynamic equilibrium.\n\nSome Formulas for Color Harmony\nThere are many theories for harmony. The following illustrations and descriptions present some basic formulas.\n\n\n1. A color scheme based on analogous colors\n\nExample of an anaologous color harmony  \n\nAnalogous colors are any three colors which are side by side on a 12-part color wheel, such as yellow-green, yellow, and yellow-orange. Usually one of the three colors predominates.\n \n\n2. A color scheme based on complementary colors\n\nExample of a complementary color harmony \n \n\nComplementary colors are any two colors which are directly opposite each other, such as red and green and red-purple and yellow-green. In the illustration above, there are several variations of yellow-green in the leaves and several variations of red-purple in the orchid. These opposing colors create maximum contrast and maximum stability.\n \n\n3. A color scheme based on nature\n\n color harmony in nature\n\nNature provides a perfect departure point for color harmony. In the illustration above, red yellow and green create a harmonious design, regardless of whether this combination fits into a technical formula for color harmony.\n\n\nDynamic recipes for color harmony\nAn e-Course from Jill Morton, Color Matters author & consultant.\n\nColor Context\nHow color behaves in relation to other colors and shapes is a complex area of color theory. Compare the contrast effects of different color backgrounds for the same red square.\n\n \n\n\n\u00a9Color Voodoo Publications\n\nRed appears more brilliant against a black background and somewhat duller against the white background. In contrast with orange, the red appears lifeless; in contrast with blue-green, it exhibits brilliance. Notice that the red square appears larger on black than on other background colors.\n\n\nDifferent readings of the same color\n\n\n\u00a9Color Voodoo Publications\n\nIf your computer has sufficient color stability and gamma correction (link to Is Your Computer Color Blind?) you will see that the small purple rectangle on the left appears to have a red-purple tinge when compared to the small purple rectangle on the right. They are both the same color as seen in the illustration below. This demonstrates how three colors can be perceived as four colors.\n\n\n\n\nObserving the effects colors have on each other is the starting point for understanding the relativity of color. The relationship of values, saturations and the warmth or coolness of respective hues can cause noticeable differences in our perception of color."
  },
  {
    "id": 3,
    "name": "curbing_power_of_popes.txt",
    "content": "Curbing the Power of the Popes\nThe survival of the papacy has always been dependent on a precarious balancing act between the pope\u2019s religious and secular powers.\n\nIn September 1870 concerned and curious Romans gathered at St Peter\u2019s Basilica. Beyond the walls of the Vatican, Pope Pius IX claimed to have been imprisoned in his own realm. For more than a millennium popes had governed territory across central Italy from their capital in Rome. When, in 1861, King Vittorio Emanuele II had united the Italian states to create the nation of Italy, the rump of the Papal States remained distinct. On 20 September 1870, however, the Italian king took the papal capital as his own. For those who questioned the popes\u2019 right to hold worldly authority, it was a just and timely act. For others, the king violated an office sanctioned by God. As a champion of tradition, Pius IX made his feelings plain: it was the pope who had locked himself inside the Vatican Palace. He and his successors would remain \u2018prisoners\u2019 until 1929, refusing to recognise Rome as the Italian capital by taking a single step on its soil.\n\nAs Italian troops streamed into the city in 1870, Pius IX appeared humiliated. On the Capitoline Hill, papal guards stood behind a barricade of mattresses. One hawker sold straw apparently slept upon by the imprisoned pope. In 1929 the pontiffs would regain their status as temporal rulers of the Vatican City state, but even then the territory granted to them did not extend much further than their self-imposed jail. Nonetheless, the history of the Papal States suggests that losing extensive territories may have been a gift rather than a blow to the papacy. The purpose of the popes\u2019 sovereign status had never been riches or renown \u2013 though some popes relished their worldly prestige. Rather, the pontiffs\u2019 temporal power was a means of guaranteeing their independence, free from obligation to secular rulers. When popes were reinstated as sovereigns in 1929 they secured their ability to run the Church as subjects only to God. As rulers of the world\u2019s smallest state, they were liberated from the politicking, scandals, and wars that had blighted their predecessors.\n\nHallowed ground\nFrom the popes\u2019 emergence in antiquity, their tie to territory was inextricable from their religious role. For most Romans visiting the Vatican Hill in the first and second centuries, however, it was far from an edifying spot. Remote from the pagan city\u2019s religious, political, and commercial centre, Tacitus tells us, \u2018promiscuity and degradation throve\u2019. In the hearts and minds of Rome\u2019s first Christians, though, the Vatican was already hallowed ground. In the first century, Saint Peter had died there. While most Romans would have seen him as an anonymous Galilean criminal, Christians saw a martyr whom Christ had elected to lead his Church on Earth. Peter\u2019s death in Rome meant that the first leaders, or bishops, of Rome\u2019s Christian community could call themselves his successors. Rome and the Vatican Hill became fundamental to the claim that the bishops of Rome \u2013 later known as popes \u2013 are the leaders of the entire Christian Church.\n\nAfter the emperor Constantine legalised and sponsored Christianity in 313, the link between Peter, his papal successors, and Rome was memorialised in marble and gold. Until then, popes had fulfilled their office in relative obscurity. Now they were installed in splendour at the Lateran, the first Western Christian basilica. On the Vatican, Constantine subsumed the humble shrine to Peter into a vast cathedral inscribed with the imperial name. He gave privileges to priests across the empire, while Rome\u2019s popes enjoyed their own palaces. Soon the popes\u2019 worldly status was so enviable that a pagan aristocrat joked: \u2018Make me the Bishop of Rome and I will become a Christian immediately!\u2019\n\nThe legalisation of Christianity meant that popes and bishops elsewhere could lead and expand the Church. However, the popes\u2019 new status in Rome and the empire encumbered them with new civic responsibilities. By 324 concerns about territories in the Eastern Roman Empire shifted Constantine\u2019s focus from Italy to Byzantium. From a new capital, Constantinople, he would rule the Eastern empire, leaving Rome and its environs under the emperors of the West, many of whom were weak and unscrupulous. For Germanic tribes seeking wealth and influence, Italy and Rome became tantalising, vulnerable targets. Rome was sacked time and again. Pelagius, a monk who fled in 410, claimed that the city, once \u2018the mistress of the world\u2019, now \u2018shivered, crushed with fear\u2019. While the Western emperors kept their courts in the relative safety of cities such as Ravenna and Milan, the popes remained in Rome, feeding the hungry, sheltering the displaced, and repairing the city. They not only took on administrative and financial responsibilities, they also filled a vacuum as the people\u2019s figurehead and pastor. When Attila the Hun marched his men into Italy in 452 it was Pope Leo I, not the Western emperor, who rode out to bargain with the man known as the \u2018scourge of God\u2019.\n\nYet despite increasing temporal responsibilities the popes of late antiquity did not gain commensurate political influence. Worse still, the Eastern Roman emperors undermined their religious role. In 476 the last emperor of the West was deposed by the Germanic general Odoacer, who made himself king of Italy. From the early sixth century Constantine\u2019s successors, the Eastern emperors in Byzantium, reclaimed much of the Western empire. They ruled Rome and its environs through delegates called exarchs based in Ravenna, and interfered directly in religious matters. Emperors pushed candidates in papal elections, demanded taxes from the Church, and mandated religious change. In 692 Emperor Justinian II tried to force Pope Sergius I to accept married priests. In 726 Emperor Leo the Isaurian banned the veneration of religious icons, sparking a riot in Constantinople. In a Rome replete with holy images, Pope Gregory II refused to implement the ban. His successor, Gregory III, also defiant, used onyx columns presented to him by the emperor\u2019s exarch to display icons. Far from yielding to this resistance, emperors went on the offensive. Justinian II demanded that Sergius be arrested and tried in Constantinople; one of Emperor Leo\u2019s exarchs planned to censure or even kill Gregory II; Leo himself seized estates from the Church.\n\nThese popes were bold as they knew that their religious office was meaningless without independence. Moreover, they claimed divinely sanctioned authority and recognised their popularity over the emperors: Sergius was saved by militias, while Gregory II\u2019s pontificate saw a sweeping rebellion against Byzantine rule. In the early eighth century the threat of yet another invading army \u2013 the Lombards \u2013 pushed the popes to find more powerful allies and make a more decisive break with Byzantium. When the Lombard king Liutprand seized Sutri, a strategic position on the border of the Roman duchy, a pope would once again ride out to meet the invader. In 728 Gregory II persuaded Liutprand to relinquish the territory as \u2018a donation to the blessed Apostles Peter and Paul\u2019, circumventing the Eastern emperor. The popes\u2019 relationship with the Lombards would prove rocky, yet their negotiations led to the clear recognition of papal rule in and outside of the Roman duchy. Seeking a more stable alliance in the 750s, Popes Zachary and Stephen II looked north to the recently converted Franks. The Frankish king Pepin the Short and his son Charlemagne would drive out the Lombards. Moreover, they recognised the popes as the governors of a huge swathe of Italy, stretching up to the River Po. Their treaties made no mention of the Byzantine authorities who nominally ruled the land.\n \n\nPolitical popes\nWith the keys to cities across Italy laying on Saint Peter\u2019s tomb in Rome, the popes became sovereigns of the Papal States and autonomous from the Byzantine emperors. Ironically, their new status embroiled them increasingly in secular matters, even as they sought to protect their religious office. Both Liutprand and Pepin knew the political benefits of posturing as guardians of the papacy. Through his alliance with Pope Zachary, Pepin was able to legitimise his contested position as king of the Franks. Stephen II went further, granting Pepin and his heirs the additional title \u2018Patrician of the Romans\u2019. In 800 Pope Leo III crowned Pepin\u2019s son Charlemagne the first Holy Roman Emperor.\n\nThough the Franks were their official protectors, the pontiffs had to work with the Roman aristocracy and lords elsewhere to administer their Papal States. Moreover, the popes themselves were often entangled with the temporal actors who sought to influence and even choose the man who sat on Saint Peter\u2019s throne. There had been violent tussles for the papacy since Constantine\u2019s sponsorship of the Church. By the early medieval period the popes\u2019 influence was widely recognised and conclaves and entire papacies became marred by bribery, scandals, and brawls. In the tenth century Liutprand of Cremona claimed that Teofilatto, Count of Tusculum, and his wife, Theodora, used their daughter to secure their influence by having her seduce Pope Sergius III, bearing him a son who would become Pope John XI. Liutprand may have exaggerated Teofilatto\u2019s misdeeds; he was an ally of their rival, Otto of Saxony. Still, other stories from the period paint an equally seedy picture. Pope John XIII had the prefect of Rome stripped naked and paraded through the city on the back of a mule as punishment for opposing his election.\n\nReforms instigated by Pope Gregory VII in the 11th century sought to enforce discipline and assert the popes\u2019 supremacy over political actors, yet papal politicking continued. Despite the popes\u2019 long history in Rome, the people of the city were far from indifferent to their power plays. In 1143 a revolution broke out when Pope Innocent II undermined a Roman plan to take the nearby hill town of Tivoli, cutting his own deal to protect the inhabitants in exchange for their fealty. The Romans demanded that the pope be stripped of all political authority and revived the ancient senate on the Capitoline Hill. Innocent took to his bed and died. His successors only re-established themselves in Rome 45 years later, with the help of the Holy Roman Emperors. When Pope Clement III finally returned in 1188 he had to accept the continued presence of the senate that represented the Commune of Rome. This was not the last time that Romans would protest the politicisation of the papacy. In the 14th century a power struggle between Pope Boniface VIII and King Philip the Fair of France led to the death of the pope (after being imprisoned by Philip\u2019s men) and the transfer of the papacy to Avignon. Noble families in and around Rome seized the opportunity to extend their influence, leading to bloodshed and chaos. Absent from Rome for 60 years, the popes were blamed. Authors such as Dante and Petrarch claimed that the pontiffs had become pawns of the French king, debasing their office and reducing Christ\u2019s Church to a \u2018Babylonian whore\u2019.\n\nComeback\nIt was a happy day for many when a pope was restored to Rome in 1420. Martin V, a member of the city\u2019s powerful Colonna family, rode into Rome on horseback flanked by jesters and roaring crowds. The years preceding Martin\u2019s election had further undermined the papal office. When cardinals had met to elect a pope in 1378, following the death of Gregory XI, a baying mob had looted the Vatican Palace, threatening to \u2018kill and cut to pieces\u2019 the cardinals voting inside. After the conclave elected Pope Urban VI, a large contingent abandoned him, declaring that they had acted under duress. The claim sparked the Great Western Schism, which saw three popes ruling at the same time, in Rome, Avignon, and Pisa. The fact that it took a council of bishops, rather than the usual conclave of cardinals, to elect Martin V also harmed papal authority, enthusing \u2018conciliarists\u2019 who argued that synods of bishops held equal or even higher authority than the pope.\n\nSoon, however, the popes appeared to have made a strong comeback, in Rome and the Papal States. With the help of family and allies, Martin V subdued the feudal lords who had acted like princes in the popes\u2019 absence. His successors would go to war with the Italian authorities that had encroached on papal lands. After 1495 the popes would even hold their own against major international powers, as France and the Spanish Habsburg dynasty vied for hegemony alongside local allies in the Italian Wars. Some popes seemed born for the role of defenders of the Papal States. In 1495 Alexander VI allied with the Holy Roman Emperor, Republic of Venice, and Ferdinand of Aragon. Capitalising on anti-French sentiment, he rolled back the French occupation of the Kingdom of Naples, protecting its status as a papal fief. Pope Julius II was even more formidable. In 1511, at the age of nearly 70, he led his army through thick snow to defeat French forces at the Siege of Mirandola. Writing in 1438 the humanist Lapo da Castiglionchio the Younger described the papal court as a centre of international power, writing that \u2018all things are deferred to the pope\u2019. The claim was somewhat exaggerated for Capo\u2019s day and later popes\u2019 political influence fluctuated. However, during the 16th and 17th centuries the alliance and blessing of the pontiff were sought via embassies from as far afield as the Congo, while popes sent out their envoys to cities from Moscow to Alexandria.\n\nThe pope diminished\nDespite the restoration of papal authority in Italy, popes could not escape the innate conflict between their religious and temporal roles. Many failed to live up to the complex demands of dual authority. Before his election in 1523 Pope Clement VII was respected as the shrewd second-in-command to his cousin, Pope Leo X. But when he became pope, Clement wavered in indecision \u2013 with disastrous consequences. When the Habsburg Holy Roman Emperor, Charles V, seemed overly ambitious to influence the Church as well as extend his territories, Clement turned on his ally, sparking a conflict that led to another Sack of Rome. Meanwhile objections to the worldliness of popes threatened their office more than ever. In a dialogue probably written by Erasmus in 1514, Pope Julius II is denied entrance to heaven by Saint Peter when he arrives stinking of booze, boasting of treasures, and wearing armour stained with blood. Martin Luther would also denounce the pope for having \u2018the wealth of the richest Crassus\u2019. At first, Luther only called for the reform of the Church, but his criticisms chimed with the concerns of princes weary of papal interference. Within decades dissatisfaction and dispute sparked Reformations that saw England, Scotland, Scandinavia, and parts of France and the Holy Roman Empire reject the religious and temporal authority of the pope.\n\nThe pope would keep his own states intact through the 17th and 18th centuries, but his influence over both religious and worldly matters was irreversibly diminished. This was obvious in the aftermath of the Thirty Years War (1618-48), which saw religious conflicts combine with questions of territory and sovereignty and entangled European powers from Spain to Sweden. When political leaders realised that only compromise and pragmatism could stem the bloodshed, Innocent X, a pope who refused to negotiate with \u2018heretics\u2019, was excluded from the negotiating table. The marginalisation of the popes on the world stage continued throughout the 17th and 18th centuries. Even Catholic princes such as the Habsburgs and Bourbons declined the popes\u2019 offers to mediate in disputes. Meanwhile, the ideas of the Enlightenment shook the beliefs that underpinned traditional powers. When Napoleon invaded the Italian peninsula in 1797, he did not even think it necessary to conquer the Papal States. After forcing Pope Pius VI to sign the financially crippling Treaty of Tolentino, he apparently declared that \u2018the old machine will fall apart by itself\u2019. \n\nMoral authority\nNapoleon\u2019s words must have seemed prophetic when Pius IX surrendered to Vittorio Emanuele II in 1870 and the white flag was raised over St Peter\u2019s Basilica. More than a millennium had passed since Pepin the Short offered himself as Rome\u2019s protector, and the pope had few allies. French troops had defended Pius from invasion and revolution, despite frustration with his conservatism, but they abandoned Rome in August 1870 when the Franco-Prussian War broke out. Confined to the Vatican, denuded of their lands, it seemed that Pius IX and his successors would now have a solely religious role. They endorsed new devotions and issued texts on the great moral questions of the age. Yet there was still political weight to the affection and influence that the popes enjoyed among Catholics all over the world. When Pius IX refused to give up the Quirinal Palace after Emanuele\u2019s seizure of Rome, the king declined to move in, fearing a backlash from the people, many of whom resented the dispute, known as \u2018the Roman Question\u2019, between the Italian state and the papacy.\n\nIn 1929 it was the popes\u2019 persistent prestige that helped them to win back their sovereign status. By that time, the fascist leader Benito Mussolini was prime minister of Italy. In the pursuit of honour and popularity Mussolini reconciled the Italian state and the Catholic Church through the Lateran Accords of 1929. By giving Pope Pius XI a state of around 440,000 square metres, he claimed to have succeeded where his predecessors had failed, restoring \u2018the true and fitting sovereignty of the pope\u2019. As with so many earlier political deals, the pope\u2019s negotiations with Mussolini would blight his moral authority. For the four popes who had ruled as \u2018prisoners in the Vatican\u2019 after Pius IX, however, the value of regaining sovereign status had been starkly underlined. During the First World War Pope Benedict XV had found himself powerless, his cries for peace falling on deaf ears. The Italian state censored incoming communications and undermined the pope\u2019s neutrality by requisitioning the Austrian Embassy to the Holy See. The Vatican State granted to Benedict\u2019s successor may have been minuscule, but it guaranteed the popes \u2018a liberty and independence not only actual but visible to all\u2019. Despite past negotiations with fascist powers, the papacy continues to possess a moral authority that leads political leaders to call upon popes for approval and verbal intervention. Yet as rulers of a state too small to trouble political actors, popes are spared the burdens that long undermined the religious authority at the heart of their office.\n\n \n\nJessica W\u00e4rnberg is author of City of Echoes: A New History of Rome, Its Popes and Its People (Icon, 2023)."
  },
  {
    "id": 4,
    "name": "ecological_impacts_of_transforpation_infra_development.txt",
    "content": "Abstract\nTransportation infrastructure, such as railways, roads and power lines, contribute to national and regional economic, social and cultural growth and integration. Kenya, with support from the Chinese government, is currently constructing a standard gauge railway (SGR) to support the country\u2019s Vision 2030 development agenda. Although the actual land area affected by the SGR covers only a small proportion along the SGR corridor, a significant proportion of the area supports a wide range of ecologically fragile and important ecosystems in the country, with potential wider impacts. This study used a qualitative content analysis approach to gain an understanding and perceptions of stakeholders on the potential ecological impacts of the interactions between the SGR and the traversed ecological systems in Kenya. Three dominant themes emerged: 1) ecosystem degradation; 2) ecosystem fragmentation; and 3) ecosystem destruction. Ecosystem degradation was the most commonly cited impact at while ecosystem destruction was of the least concern and largely restricted to the physical SGR construction whereas the degradation and fragmentation have a much wider footprint. The construction and operation of the SGR degraded, fragmented and destroyed key ecosystems in the country including water towers, protected areas, community conservancies and wildlife dispersal areas. Therefore, we recommend that project proponents develop sustainable and ecologically sensitive measures to mitigate the key ecosystem impacts.\n\nIntroduction\nThe contribution of transportation infrastructure to economic, social and cultural growth through regional and global integration has had mixed outcomes [1\u20133]. There has been unprecedented growth and expansion of transportation infrastructure, particularly in sub-Saharan Africa, accounting for up to 90% of new infrastructure including but not limited to railways, roads and power lines [4, 5]. Recently, many African governments have increasingly favoured railways due to their economic and environmental advantages [6]. Since their inception, railway technology has evolved considerably and reduced transportation costs and increased safety not only for passengers, railway workers and freight but also for the human populations living within the railway corridors [7, 8]. Despite the advantages, and the need to meet the increasing demand for environmental accountability, there is a growing recognition of their impacts on the natural environment, especially in remote and fragile ecosystems characterized by low human population, poor and marginalized communities and marginal or changing climatic conditions. Furthermore, although railways may share impacts with other anthropogenic activities, they have unique impacts associated with their linear form constituting \u201cdisturbance corridors\u201d that disrupt the natural, more homogeneous landscape [9: 157]. Consequently, new calls are emerging to identify, quantify and mitigate the impacts of railways and other transportation infrastructure [4, 10\u201315]. The potential ecological impacts of the railways can be captured through stakeholder perceptions, and that insight used to inform on ecologically sensitive design, implementation, and mitigation of linear infrastructure impacts.\n\nThe rapidly expanding transportation infrastructure can impact the environment both directly, as an immediate consequence of the presence of the infrastructure and its construction or indirectly, as a result of human activities that are facilitated by new infrastructure [13]. An analysis of the impacts of 33 planned and existing development corridors, including transportation infrastructure in sub-Saharan Africa showed that ecosystems have been significantly impacted by increasing land-use pressure and encroachment that has in part been triggered by the infrastructure development [13]. Other studies have established that transportation infrastructure leads to loss of ecosystem integrity through truncation of ecosystems into smaller, often isolated, patches that may not be able to maintain or sustain ecological processes in the long run [16]. Such impacts include bisection of watersheds and basins; physical disturbance and disruption to the composition, structure and functioning of ecosystems, and movement, migration and survival of resident wildlife species [13, 14, 17]; direct mortality of wildlife through vehicle/train\u2013wildlife collisions (V/TWC) [10, 16, 18], and behavioural modification among diverse species in different ways [19]. Furthermore, it is argued that transportation infrastructure contributes to soil, water and air pollution [20]; alteration of natural processes including natural hydrology, fire regimes and competitor and predator-prey relationships among other impacts [21\u201325]. According to Hulme [26] and Catford et al. [27], transport infrastructure may act as migration corridors for the natural dispersal of non-native biodiversity by allowing their movement across physical and environmental barriers or by supplying suitable habitat for their expansion.\n\nThe most severe but rarely reported impact of transportation infrastructure is the destruction and loss of natural ecosystems [28]. Ecosystem destruction is the absolute loss of habitat surface area through the physical presence of a road or railway and related facilities that can take up a substantial amount of space [20]. The construction of transportation infrastructure often results in land use conversion, from natural ecosystems such as forests and water bodies, to transportation land use or right-of-way [21, 29]. This entails clearing of vegetation and the accompanying levelling operations that destroy the original topography and soil profile. It further entails the elimination, replacement or modification of the original characteristics of natural vegetation and aquatic ecosystems [20, 21].\n\nPrevious research on the ecological impacts of infrastructure has focused largely on roads where a considerable effort has been expended to quantify their effects particularly in Europe, North America and Australia [30]. Although railways share similar ecological impacts with other transportation infrastructure, little research has been undertaken. Hence, less is known about the direct and indirect impact of railways on ecological communities and processes. Understanding the impacts of railways and the associated rail traffic is important as part of Kenya\u2019s commitment to international initiatives for the protection of biological diversity, supporting the evaluation of the effectiveness of impact mitigation measures, and to support cumulative environmental assessment and transportation planning.\n\nTo characterise and assess these impacts, qualitative content analysis was used to capture the rich and deep narratives from a wide range of stakeholders [31]. Qualitative data was collected from 19 group interviews and meetings comprising 54 key informants from 14 sites along the SGR phases 1 and 2A stretching from the Kenyan coastal city of Mombasa to Suswa in Rift Valley. To ensure we captured the wide range of impacts and benefits we emphasized trust, transparency, verifiability and flexibility in our method [32, 33]. We used Qualitative Data Analysis Miner Lite (QDA) software to code and categorize the data. ArcGIS 10.4 was used to spatially map the SGR, key ecosystems and protected areas; the GIS was queried together with the qualitative data to identify impact hotspots.\n\nApproach and methodology\nThis study was approved by the Ethics Review Group of the UNEP-WCMC and the ESRC following the Code of Practice on Ethical Standards in Research. The protocols used in the study were approved by the National Council for Science and Technology and Innovation of the republic of Kenya (Permit No. NACOSTI/P/19/3232/27585). A total of 54 state and non-state officials were interviewed between January and May 2019 along the entire SGR Corridor in southern Kenya. Informed consent was sought according to the UNEP-WCMC and ESRC Research Ethics guidelines and strategies aimed at minimizing harm to the subject.\n\nStudy area\nThe study was conducted along the entire stretch of the Kenya Standard Gauge Railway (SGR) Phase I & Phase IIA, covering eight counties from Mombasa to Narok (Fig 1). The SGR has been billed as the biggest transport infrastructure in the country\u2019s history under the Vision 2030 development agenda [34]. The SGR forms part of the East African Railway Master Plan (2009) and the Eastern African SGR regional network which aims to rejuvenate existing railways serving Tanzania, Kenya, Uganda and extend to Burundi, Rwanda, Ethiopia and South Sudan [35]. The SGR runs westwards from the coastal town of Mombasa and through the central Kenya with the line through western Kenya to Malaba town at the Kenya-Uganda border still under construction to link up with other standard gauge railways that are being built in East Africa [35]. The construction of the SGR begun in 2014 at an estimated cost of US$3.8 billion, with 90% supplied by a loan from the Exim Bank of China and 10% coming from the Kenyan government [34].\n\nFig 1. Map of SGR corridor and different resources.\nFig 1\n\nOpen in a new tab\nThe prime contractor on the railway was the China Road and Bridge Corporation [5]. The construction of the SGR has been undertaken in phases: the first phase from Mombasa to Nairobi was completed and has been in operation since May 2017, while the second phase from Nairobi South Railway Station to Naivasha Industrial Park in Enoosupukia and onto Narok town was completed in August 2019. Meanwhile, the third phase covering Narok to Kisumu and onto Malaba is yet to be constructed [5].\n\nThe SGR is categorized as a National Class I railway and has a wide range of safety protection measures in the design and operation that include speed limits, installation of high guard fence, safety buffers and earth embankments to avoid crossing other infrastructures. Furthermore, bridges, underpasses, culverts and flyovers have been constructed in wildlife areas such as Tsavo and Nairobi National Parks and in high human density areas such as Athi River to facilitate free movement of wildlife and people. Within Nairobi National Park, an acoustic noise barrier has been installed to reduce noise disturbance to wildlife [36, 37]. Despite the measures taken to minimise the impacts of the SGR, anecdotal reports point to the existence of negative impacts on the ecological communities and processes along the SGR corridor.\n\nData collection\nThe study gathered insights and perspectives from group interviews and meetings with a diverse range of stakeholders. Individual interviews were also carried out with experts who were either working alone or had colleagues out in the field. Interview questions revolved around the description of their mandates and interaction with the SGR and participants\u2019 perceptions. The participants were drawn from the corridor institutions such as Kenya Railways Corporation and Kenya Ports Authority (n = 20), government institutions mandated with natural resource conservation and management such as the National Environment Management Authority, Kenya Forest Research Institute, Water Resources Authority and the Kenya Wildlife Service (n = 10), community groups such as Community Forest Associations, local farmers and pastoralists (n = 10), non-governmental and research organisations (n = 4) and three county governments along the SGR (n = 10).\n\nAll the interviews were recorded and reviewed at the end of the day. In addition, all the activities within and around the SGR installations, the status of surrounding landscape, resources and participants\u2019 conversations were observed, captured and recorded for further analysis. The observations focused on visible and verifiable conditions of the landscapes that could be or were linked to the SGR activities such as construction and maintenance as well as the relationship and behaviour of stakeholders while discussing cross-cutting issues.\n\nData analysis\nQualitative content analysis was used to identify the potential ecological impacts of the SGR based on the perceptions of stakeholders. This interpretive process focuses on both the subject and background and explores the similarities and differences between and within different parts of the text [38]. Computer-Assisted Qualitative Data Analysis software (CAQDAS) called Qualitative Data Analysis (QDA) Miner\u00ae [39] was applied to code and categorize the qualitative data into thematic areas, and preliminary codes that were based on the literature [40] and appropriate categories identified through the analysis of the field data. Codes and units of meaning were interpreted in the context of the study and compared for similarities and differences (see S1 and S2 Appendices). All the geo-referenced data were transferred to the Geographic Information System (GIS) for spatial analysis using ArcMap in ArcGIS 10.4 version [41].\n\nResults on the ecosystem impacts of the SGR\nThe results of our analysis and inspection of codes and subclasses resulted in the extraction of three main themes: ecosystems degradation, ecosystem fragmentation and ecosystem destruction.\n\nEcosystem degradation\nEcosystem degradation emerged as the main category of impact during the meetings and consisted of three subcategories.\n\nContamination of soil, water and air\nParticipants in most of the meetings identified issues around soil, water and air contamination during construction and operation of the SGR. Expected impacts of the SGR included population growth through migration, shifts in land use and land tenure and the emergence of illegal activities such as illegal grazing in protected areas. Officials of the Kenya Wildlife Service observed that \u201clocal communities [were] using the underpasses to pass their livestock through to Tsavo National Park particularly around Buchuma gate\u201d. The livestock incursions they observed \u201cresulted in serious soil degradation in the southern part of Tsavo East\u201d. Other concerns included oil spills as observed by local officials in Kibwezi County \u201cpollution of the Thange river [though oil spill of 2015 and a recent one in May 2019 around Machakos that contaminated Athi River] has had a great impact on the community given that it provides water for cultivation which is the main economic activity in the area. Since the oil spill incident, the use of the [Thange] river for irrigation, livestock watering, and domestic purposes have been suspended and the land in the affected area is still unsafe for cultivation\u201d.\n\nParticipants also observed that noise pollution during construction and operation of the SGR were common in the areas around Nairobi and Voi. Officials in Voi stated that \u201cthe main environmental impact currently observed [by the KRC staff] is noise pollution when the trains are passing, while local communities around Nairobi county reported impacts of \u201cblasting for construction materials causing tremors in the area and leading to buildings cracking, for example, at Oloosirkon primary school. Meanwhile, dust pollution [was] also a challenge and impacts include infections from dust, coughs and chest pain\u201d (Fig 2).\n\nFig 2. A new water body with high suspended sediment load in an abandoned quarry (a) and dust pollution during construction in Tuala area (b).\nFig 2\n\nOpen in a new tab\nSoil erosion, sedimentation and flooding\nThis was mainly reported in areas along the coast where local Community Forest Association officials observed that sediments eroded from the rail embankments \u201cdid not only affect mangroves seed development and self-germination but also blocked streams and reduced the stream size in Kilindini\u201d. Meanwhile, respondents from Narok county observed that directing water to the underpasses led to gulley erosion (Fig 3A) affecting soil cover and leading to siltation of Lake Magadi. In Voi, county officials observed that \u201cstorm water directed to the culverts flooded low lying homesteads and farms during heavy rains\u201d. Furthermore, respondents from Nairobi and Narok Counties reported incidents of \u201cflooding along the culvert [underpasses] when it rained while rivers [Empakashe and Mbagathi] had been blocked or dried up completely because they had been filled with silt from the construction\u201d. Other areas affected by flooding due to redirected water included Kibarani, Kiunduani, Ngwiw\u2019a, Mutantheeu, Emali town and Kima in Makueni County.\n\nFig 3. A damaged stream in Kitengela (a) and an eroded area near an underpass in Duka Moja (b).\nFig 3\n\nOpen in a new tab\nIntroduction and spread of invasive plant species\nParticipants from Voi reported that invasive plant species had recently emerged and were spreading rapidly along the SGR corridor. This, they observed \u201cwas a problem in both [Tsavo East-West] National Parks where the invasive cactus Opuntia stricta and Prosopis juliflora (known locally as Mathenge) was also prevalent along the new highway from Voi to Taveta\u201d. However, we could not establish the connection between the spread of invasive plant species and the construction and operation of the SGR. Meanwhile, in some areas in Kajiado county, invasive plant species were regenerating after long periods of absence due to what locals observed as \u201cdisturbance through borrow pits and truck tracks taking soil to the SGR construction sites\u201d. Although our study team made clear observations of the presence of these invasive plant species, we could not verify that they resulted from the construction and operation of the SGR. This was because there were no clear timelines or monitoring on the emergence and spread of these species linking them to the construction activities.\n\nEcosystem fragmentation\nEcosystem fragmentation emerged as the second dominant theme from our consultative meetings. The SGR traversed key ecosystems and resources creating a barrier to the movement of terrestrial animals and reducing sizes of some ecosystems and resources. Participants in the meetings raised concerns that \u201cthe infrastructure [had] also been seen to affect wildlife movements, for example with animals congregating along the highway\u201d. To mitigate this the SGR contractors provided underpasses and bridges to allow wildlife to move freely. However, local communities have settled within these underpasses and under bridges. During this study, illegal settlements were observed along the SGR section bisecting Tsavo East and West National parks, further blocking wildlife movement across the SGR. This was further reinforced by meeting participants who observed that \u201ca lot of underpasses have been blocked by the proliferation of illegal settlements and the conversion of land to agriculture\u201d. Due to the sustained construction activities along the SGR and its use, most animals, especially elephants were observed to have changed their behaviour and responses as observed by one local leader that \u201csome animals such as elephants have become more aggressive as they interpret traffic noise as an indicator of the presence of humans and consequently appear to be on the defensive\u201d. One of the consequences of the barrier effect is the emergence and intensification of human-wildlife conflicts (HWC) as animals move away from railways and roads to surrounding communities, thereby reducing the buffer between wildlife areas and human communities. Supporting this observation, some respondents confirmed that \u201cthere had been an observed increase in human-wildlife conflict which may not necessarily be attributed to the SGR\u201d.\n\nEcosystem destruction\nDestruction of the ecosystem due to the SGR and related activities was the least dominant issue of concern to our participants. The construction of the SGR together with the associated quarrying activities resulted in the removal of forest or vegetation cover, destruction of water sources such as rivers, wetlands, grasslands, and parts of protected areas Participants observed that activities along the SGR \u201cmodified or disturbed the natural ecosystems especially low-lying, poorly drained land in areas around Kibwezi, Mombasa ad Voi\u201d. Of concern was the conversion of land [agricultural and grazing] to settlement and real estates. During a meeting in Kibwezi, the participants stated that they \u201chad observed a number of changes in land use in the area, mainly sub-division of land due to population pressure and development of the area through the railway and road network\u201d. Whereas participants in Narok noted that \u201creal estate development was picking up in Suswa. In addition, land sales were high especially around the proposed [SGR] station in Suswa, mainly in anticipation of the [Naivasha] dry port\u201d. Our team observed that wetlands around Kitengela and Kiboko were blocked off and damaged, thereby affecting natural water flow (Fig 3B). Similarly, we observed that forests in Kibwezi at the KEFRI station had been cleared to create way for the construction of the railway.\n\nEcosystem degradation, fragmentation and destruction\nAlthough the actual land area directly affected by the SGR may only cover a small area along the corridor, a much wider area supports a wide range of ecologically fragile and important ecosystems in Kenya. The SGR has clearly had some direct ecological impacts emanating from its construction and subsequent operation. In particular, the SGR has contributed to wider ecosystem degradation and fragmentation. Although similar impacts have been reported by road and railway ecologists through quantitative scientific approaches in different contexts, this study demonstrates that these impacts can be understood by engaging with a range of different stakeholders from diverse backgrounds, that bring together a range of experiences and expertise, even when and where the infrastructure development is relatively new, as is the case in Kenya.\n\nThe majority of participants in this study identified ecosystem degradation as the main impact around the SGR. Within the context of transportation infrastructure, degradation may arise from disturbance, pollution and contamination of soil, water and air and disruption of natural processes [20]. The SGR construction was accompanied by activities such as soil compaction, excavation and movement of soil from one location to another to erect the embankments. These activities altered and created barriers to natural processes including natural hydrology and animal migration routes. Previous research demonstrates how wildlife residing close to the infrastructure is affected by the traffic noise, vibrations, chemical pollution and human presence [18, 22]. Although infrastructure engineers rely on technical considerations when designing and implementing these projects, local knowledge and experiences of critical ecosystems can contribute to the identification and protection of these ecosystems. In areas where engineers and scientists have no prior knowledge and experience of potential impacts, stakeholder perceptions can play an important role in providing baseline information to design, implement and monitor impact mitigation activities.\n\nAlthough little scientific research has been conducted on the changing nature of wildlife behaviour and railways, especially in Africa, we found that elephants had displayed early signs of behavioural modification in response to the activities around and within the SGR corridor particularly around Tsavo in Voi. These are consistent with behavioural adaptations observed among other species including shifting their home ranges or altering their movement patterns away from areas with high infrastructure densities (e.g. [42\u201344]). Barnes et al. [45] reported that elephants (Loxodonta africana) in north-eastern Gabon preferred sites away from both roads and villages. However, this does not hold true for all species, for example, Coleman and Fraser [46] reported that black vultures (Coragyps atratus) and turkey vultures (Cathartes aura) preferred home ranges in areas with greater road densities perhaps due to availability of food. The ability of stakeholders to articulate these provides a key signal towards systematic monitoring of such changes by railway ecologists in Kenya.\n\nThe SGR, by its linear form, cuts across watersheds and drainage basins thereby altering and modifying the local natural hydrological environment. The SGR contractors rerouted or concentrated surface runoff to the underpasses, inevitably increasing the volumes and speed of the flow. This has resulted in flooding, soil erosion, channel modification, and siltation of streams. A significant environmental impact in the SGR is soil erosion, sedimentation and resultant flooding. Indeed, clearing vegetation along and adjacent to the infrastructure can lead to soil erosion and sediment inputs to watercourses [47]. These degraded ecosystems might depict altered microclimatic conditions with risks of flooding and landslides [2]. Meanwhile, underground water sources can be degraded by runoff and hazardous material spills that contaminate aquifers [25]. Furthermore, our study raises concern about the use of embankments to raise the SGR as this inevitably results in rerouting of surface runoff to avoid cutting through the earth embankments. These concerns challenge the conventional infrastructural engineering designs that rely on data and experiences from other areas. The SGR design, alignment and routing was based on existing designs and expertise from China. This approach can draw parallels with challenges around technological transfers that do not take into account local ecological and social context. Engaging with local communities and stakeholders can offer insights into local and ecologically sensitive designs with minimal impacts.\n\nThe introduction and spread of invasive plant species due to the recent large scale expansion of the transportation infrastructure have become a major global concern [48\u201350]. As suggested by Hulme [27] and Catford et al. [26], railway infrastructure can act as a corridor for the natural dispersal of non-native plant species as was observed in different locations along the SGR corridor. Studies across Africa, US and Europe have shown that railway verges and embankments host a high diversity of non-native species [51\u201353]. In most cases, railway verges are regularly mowed or cleared resulting in open spaces suitable for invasive species [51, 54]. This creation of new habitats opens up new niches that can be exploited by invasive plant species that can migrate quickly along the linear infrastructure. Early detection through consultations with local communities and stakeholders can offer opportunities for effective management and eradication.\n\nSome of the railway impacts required long-term monitoring to establish discernible patterns such as physical disturbance and disruption of the continuous vegetative community, structure and function of ecosystems, and movement, migration and survival of resident wildlife species [13, 14, 17]. Such disruptions may result in isolation of populations, gene flow restrictions and loss of biodiversity [13, 17, 22, 55, 56]. Furthermore, studies elsewhere have reported mortality of a variety of mammal species such as grizzly bears (Ursus arctos) [57], moose (Alces alces) [58], elephants (Loxodonta spp) [59\u201363] and frogs (Anura spp) [64] for decades. These studies have involved long-term systematic monitoring and reporting of the impacts. Given the short span between construction and operation of the SGR, it is not surprising that participants in this study only reported fewer cases and had no clear appreciation of the long term impacts. However, stakeholder input offered a clear need for more systematic monitoring of these impacts to ascertain their occurrence and severity in Kenya.\n\nThe reduction or destruction of ecosystems and replacement with non-natural habitats is a key component of infrastructure development [20]. Although the SGR contractors undertook post-construction rehabilitation of the degraded environment, the original natural characteristics of the land were eliminated, replaced or modified, and there is a significant impact on the landscape of railway infrastructure [21, 29]. Communities relying on wetlands and rivers in Voi, Kibwezi, Tuala and Narok areas lost access to these critical resources, and it is not yet clear how other ecosystem services could be lost in the longer term. Clearly the infrastructure impacts on ecosystems will impact on the value and delivery of natural capital, particularly those associated with water and wildlife impacts.\n\nThe discussions above point to the existence of negative impacts of the SGR to the ecological systems along the corridor. These impacts would normally be identified and effective mitigation measures put in place during and after the construction of the SGR through Environmental and Social Impact Assessment (ESIA). During this study, we confirmed that ESIA for the two phases of the SGR were conducted and final reports written to facilitate licencing by the National Environment Management Authority (NEMA), the government regulator [36, 37]. Kenya\u2019s ESIA framework is fairly comprehensive and is anchored on sound legislative and institutional set-up, designed to protect both the social and ecological systems and emphasizes public participation in the ESIA process [65]. However, the persistence and emergence of potential ecological impacts coupled with the likely ineffectiveness of mitigation measures outlined by the participants in this study, point to challenges with public participation and little oversight both in the ESIA process and implementation of the development projects.\n\nConclusion\nThe construction of the SGR has led to major impacts on ecosystem, particularly degradation fragmentation and to a less extent ecosystem destruction. Landscape modification by the SGR construction has resulted in increased soil erosion, land degradation, flooding, sedimentation of water bodies, habitat destruction and impeding wildlife movements. It is therefore recommended that linear infrastructure projects, like the SGR, develop sustainable and ecologically sensitive measures to mitigate against these impacts. For example, underpasses at the right density and of the right size will maintain wildlife movements, water courses can be channelled and redirected, invasive plants may need to have direct control measures implemented to try and stop their spread. Furthermore, a wholesome assessment of the environmental impacts of transportation infrastructure which involves extensive engagement of stakeholders is key for designing and implementing inclusive, resilient and sustainable infrastructure so that the development gains are maximised while the ecosystem impacts are minimised. Further research is needed to quantify the ecological impacts of the SGR, and other transport infrastructure on ecosystems and the associated natural capital with a view of establishing mitigation measures that would promote more sustainable and resilient developments in future."
  },
  {
    "id": 5,
    "name": "globalized_anime_industry.txt",
    "content": "Anime & Manga Explained\n\nThe anime/manga industry is a durable business that has survived past threats to its extraterritorial growth, and continues to confront challenges to this day. Before getting into detail about the expansion of this multi-organizational commerce, it is important to understand what anime and manga are. Manga, originating in Japan, are \u201ccomic books and graphic novels\u201d (Otmazgin, 2014, p. 57) that read from right-to-left. Nissim Otmazgin cites Gilles Poitras (2008) in describing anime as \u201cproducts of artistic interaction between Japanese, European and American cultures, combined with new printing and media technologies\u201d (Otmazgin, 2014, p. 57). Typically, an artist composes a manga story first, and subsequently an animating organization adopts the work to bring it to life in a motion picture. Anime has a graphic design unique to Japan, as well as a character and story development style that Westerners often find uniquely attractive, especially when portrayed as an animation. Joshua Draper cites Susan J. Napier (2001) in stating \u201cJapanese animation, when compared to Western animation, is not necessarily aimed at a younger audience. In fact, a number of anime typically feature complex stories that have a more mature audience in mind\u201d (Draper, 2015, p. 8).\n\nAnime Goes Global\n\nThe origin of anime actually has Western influence, as Japanese artists would study the work of American and European composers dating back to 1914 (Draper, 2015, p. 11). Anime first came to America in 1963 when artist Osamu Tezuka\u2019s Tetsuwan Atomu was brought to NBC. This also began a challenge that would face the anime industry for decades to come, especially when aired on cable television. The title of Tezuka\u2019s craft was changed to Astro Boy (Draper, 2015, p. 12). Globalization has affected the anime industry by altering key features of the communicative medium: such as the gender of characters; display of violence; other mature content; and translated dialogue. Children were originally the target audience of anime in the U.S., therefore censorship became a norm after its first airing on television. Social information processing (Cheney, Christensen, Zorn, & Ganesh, 2011, p. 373) theorists would argue that upon NBC\u2019s successful\u2014and censored\u2014airing of Astro Boy, other companies viewed censorship as an essential attribute to anime production.\n\nAnother reason for censorship, even for anime geared toward adult audiences, was due to cultural differences. The United States televised companies did not want to air aspects of Japanese storylines that were not viewed favorably in the West, such as homoeroticism (Draper, 2015, p. 13). This is just one example of the United States fearing the potential second level effects (Cheney, Christensen, Zorn, & Ganesh, p. 377) of raw Japanese content being exposed to its society. However, this was a successful means of introducing anime to the western world. And as the fan base grew, so did the desire for uncensored content.\n\nAnime had succeeded by growing vastly in popularity in the U.S. by the early 21st Century, and with this success came a new challenge. A subculture of fans began to grow throughout the nation, and they sought to help anime reach the masses even further. \u201cThese fans were willing to help translate the original Japanese anime into their native languages in a process called \u201cfansubbing\u201d in which unofficial subtitles were added to Japanese anime by devoted fans\u201d (Draper, 2015, p. 15). Ironically, these fansubbers were not only posing a threat to the individuals who were paid to translate and subtitle the animes, but were also threatening the profits of the production studios and original artists by uploading the subtitled media onto the Internet for the world to view free of charge. Here, a diametric was introduced to the entirety of the anime culture that exists to this day: as anime grows in popularity, the eagerness of investors to make contributions declines because profits face greater risks.\n\n\u201c...the dynamic pace of anime correlates with the character of high-speed Internet and its\nappeal to the so-called \u201cInternet generation.\u201d At the same time, since Internet-based\ndistribution is mainly classed as piracy, in the long term it preempts attempts by new\nompanies from (legally) marketing anime in the US and may eventually result in\nanime\u2019s decline\u201d (Otmazgin, 2014, p. 63).\n\nWhile the industry must find a way around the threatening technological advancement of the Internet, it certainly is still a global media giant. Tai Wei Lim, Wen Xin Lim, Xiaojuan Ping, and Hui-Yi Tseng cite Roblyn Simeon (2006) in relaying \u201cAt least from 2006, Japanese anime industry already makes up more than 60% of global output of cartoons\u201d (Lim, Lim, Ping, & Tseng, 2016, p. 73). It is also important to note that anime is a popular visual entertainment commodity in countries other than Japan, the U.S., and European nations. \u201cAccording to the Association of Japanese Animations, 60 member anime production companies now provide products in 112 countries, reaching some 87.2 percent of the world\u2019s population\u201d (Nagata, 2010, at large). Masashi Kishimoto, originator of the popular manga and anime\u2014Naruto\u2014recognizes that the success of his work is largely due to the extraterritorial nature of the anime community. The artist remarked in an interview with Andrea Towers: \u201c...I must say that part of what made Naruto successful was the globalization of manga, and the fact that all over the world, fans can enjoy and read manga even though the reading order is different than what people are used to\u201d (Kishimoto, 2015, at large). Additionally, the introduction of anime to the Internet also allows Western viewers to watch their favorite anime uncensored by corporate television studio giants, like NBC.\n\n  The Future of the Anime Industry - A Rough Road Ahead\nAnime has permanently influenced American animation companies. This is seen in the creation of \u201cAmericanized\u201d animes such as Avatar: The Last Airbender. In fact, there is debate in the community as to whether or not an animated production must be created in Japan in order to be considered a true anime. Due to the growing challenge of profiting from anime, more artists are creating what is known to sell, and therefore there is a decline in originality in the industry (Draper, 2015, p. 33). However, there is hope for the anime industry, and its beacon is the Internet. Online companies such as Crunchyroll serve as \u201cthe netflix of anime\u201d. For a small monthly fee subscribers can pull from a variety of anime and manga to view. Popular anime also have ties to video-gaming and action-figure industries (Otmazgin, 2014, p. 63). Anime has endured hard times since its inception into the global market, and has risen to be a top-tier visual communication medium. Contributors to the anime culture have used advancements in technology to overcome past obstacles, such as censorship, and have the opportunity to expand further by establishing a cyber pay-to-play community, giving stakeholders a reason to stay involved."
  },
  {
    "id": 6,
    "name": "image_worth_16_x_16_words.txt",
    "content": "Published as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy\u2217,\u2020\n, Lucas Beyer\u2217\n, Alexander Kolesnikov\u2217\n, Dirk Weissenborn\u2217\n,\nXiaohua Zhai\u2217\n, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby\u2217,\u2020\n\u2217\nequal technical contribution, \u2020\nequal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.1\n1 INTRODUCTION\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\na large text corpus and then fine-tune on a smaller task-specific dataset (Devlin et al., 2019). Thanks\nto Transformers\u2019 computational efficiency and scalability, it has become possible to train models of\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\nmodels and datasets growing, there is still no sign of saturating performance.\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\ntheoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNetlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\n2020).\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\nTransformer directly to images, with the fewest possible modifications. To do so, we split an image\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\nthe model on image classification in supervised fashion.\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size. This\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\n1\nFine-tuning code and pre-trained models are available at https://github.com/\ngoogle-research/vision_transformer\n1\narXiv:2010.11929v2 [cs.CV] 3 Jun 2021\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufficient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\nfind that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\nand 77.63% on the VTAB suite of 19 tasks.\n2 RELATED WORK\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often\npre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019)\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\nNaive application of self-attention to images would require that each pixel attends to every other\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\nto apply Transformers in the context of image processing, several approximations have been tried in\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\npromising results on computer vision tasks, but require complex engineering to be implemented\nefficiently on hardware accelerators.\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 \u00d7 2\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\nuse a small patch size of 2 \u00d7 2 pixels, which makes the model applicable only to small-resolution\nimages, while we handle medium-resolution images as well.\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\nof self-attention, e.g. by augmenting feature maps for image classification (Bello et al., 2019) or by\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classification (Wu\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or unified text-vision tasks (Chen\net al., 2020c; Lu et al., 2019; Li et al., 2019).\nAnother recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\nto image pixels after reducing image resolution and color space. The model is trained in an unsupervised fashion as a generative model, and the resulting representation can then be fine-tuned or\nprobed linearly for classification performance, achieving a maximal accuracy of 72% on ImageNet.\nOur work adds to the increasing collection of papers that explore image recognition at larger scales\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-ofthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\nwell, but train Transformers instead of ResNet-based models used in prior works.\n2\nPublished as a conference paper at ICLR 2021\nTransformer Encoder\nMLP\nHead\nVision Transformer (ViT)\n*\nLinear Projection of Flattened Patches\n* Extralearnable\n[ cl ass] embedding\n1 2 3 4 5 6 7 8 9 Patch + Position 0\nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded\nPatches\nMulti-Head\nAttention\nNorm\nMLP\nNorm\n+\nL x\n+\nTransformer Encoder\nFigure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classification, we use the standard approach of adding an extra learnable\n\u201cclassification token\u201d to the sequence. The illustration of the Transformer encoder was inspired by\nVaswani et al. (2017).\n3 METHOD\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures \u2013 and\ntheir efficient implementations \u2013 can be used almost out of the box.\n3.1 VISION TRANSFORMER (VIT)\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings. To handle 2D images, we reshape the image x \u2208 R\nH\u00d7W\u00d7C into a\nsequence of flattened 2D patches xp \u2208 R\nN\u00d7(P\n2\n\u00b7C)\n, where (H, W) is the resolution of the original\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\nflatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT\u2019s [class] token, we prepend a learnable embedding to the sequence of embedded patches (z\n0\n0 = xclass), whose state at the output of the Transformer encoder (z\n0\nL\n) serves as the\nimage representation y (Eq. 4). Both during pre-training and fine-tuning, a classification head is attached to z\n0\nL\n. The classification head is implemented by a MLP with one hidden layer at pre-training\ntime and by a single linear layer at fine-tuning time.\nPosition embeddings are added to the patch embeddings to retain positional information. We use\nstandard learnable 1D position embeddings, since we have not observed significant performance\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\nsequence of embedding vectors serves as input to the encoder.\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\n3\nPublished as a conference paper at ICLR 2021\nThe MLP contains two layers with a GELU non-linearity.\nz0 = [xclass; x\n1\npE; x\n2\npE; \u00b7 \u00b7 \u00b7 ; x\nN\np E] + Epos, E \u2208 R\n(P\n2\n\u00b7C)\u00d7D, Epos \u2208 R\n(N+1)\u00d7D (1)\nz\n0\n` = MSA(LN(z`\u22121)) + z`\u22121, ` = 1 . . . L (2)\nz` = MLP(LN(z\n0\n`)) + z\n0\n`, ` = 1 . . . L (3)\ny = LN(z\n0\nL) (4)\nInductive bias. We note that Vision Transformer has much less image-specific inductive bias than\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\nat fine-tuning time for adjusting the position embeddings for images of different resolution (as described below). Other than that, the position embeddings at initialization time carry no information\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\nfrom scratch.\nHybrid Architecture. As an alternative to raw image patches, the input sequence can be formed\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\nflattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\nThe classification input embedding and position embeddings are added as described above.\n3.2 FINE-TUNING AND HIGHER RESOLUTION\nTypically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks. For\nthis, we remove the pre-trained prediction head and attach a zero-initialized D \u00d7 K feedforward\nlayer, where K is the number of downstream classes. It is often beneficial to fine-tune at higher\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\n2D interpolation of the pre-trained position embeddings, according to their location in the original\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\n4 EXPERIMENTS\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\nthat self-supervised ViT holds promise for the future.\n4.1 SETUP\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\nfollows Kolesnikov et al. (2020).\n4\nPublished as a conference paper at ICLR 2021\nModel Layers Hidden size D MLP size Heads Params\nViT-Base 12 768 3072 12 86M\nViT-Large 24 1024 4096 16 307M\nViT-Huge 32 1280 5120 16 632M\nTable 1: Details of Vision Transformer model variants.\nWe also evaluate on the 19-task VTAB classification suite (Zhai et al., 2019b). VTAB evaluates\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\nthree groups: Natural \u2013 tasks like the above, Pets, CIFAR, etc. Specialized \u2013 medical and satellite\nimagery, and Structured \u2013 tasks that require geometric understanding like localization.\nModel Variants. We base ViT configurations on those used for BERT (Devlin et al., 2019), as\nsummarized in Table 1. The \u201cBase\u201d and \u201cLarge\u201d models are directly adopted from BERT and we\nadd the larger \u201cHuge\u201d model. In what follows we use brief notation to indicate the model size and\nthe input patch size: for instance, ViT-L/16 means the \u201cLarge\u201d variant with 16\u00d716 input patch size.\nNote that the Transformer\u2019s sequence length is inversely proportional to the square of the patch size,\nthus models with smaller patch size are computationally more expensive.\nFor the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization layers (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\nconvolutions (Qiao et al., 2019). These modifications improve transfer (Kolesnikov et al., 2020),\nand we denote the modified model \u201cResNet (BiT)\u201d. For the hybrids, we feed the intermediate feature maps into ViT with patch size of one \u201cpixel\u201d. To experiment with different sequence lengths,\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\nTraining & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\n2015) with \u03b21 = 0.9, \u03b22 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\nrate warmup and decay, see Appendix B.1 for details. For fine-tuning we use SGD with momentum,\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we fine-tuned at\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\nMetrics. We report results on downstream datasets either through few-shot or fine-tuning accuracy.\nFine-tuning accuracies capture the performance of each model after fine-tuning it on the respective\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\nthat maps the (frozen) representation of a subset of training images to {\u22121, 1}\nK target vectors. This\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\nfine-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-fly evaluation\nwhere fine-tuning would be too costly.\n4.2 COMPARISON TO STATE OF THE ART\nWe first compare our largest models \u2013 ViT-H/14 and ViT-L/16 \u2013 to state-of-the-art CNNs from\nthe literature. The first comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\nperforms supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,\n2020), which is a large EfficientNet trained using semi-supervised learning on ImageNet and JFT300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and\nBiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\nv3 cores (2 per chip) used for training multiplied by the training time in days.\nTable 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computational resources to train. The larger model, ViT-H/14, further improves the performance, especially\non the more challenging datasets \u2013 ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\n5\nPublished as a conference paper at ICLR 2021\nOurs-JFT Ours-JFT Ours-I21k BiT-L Noisy Student\n(ViT-H/14) (ViT-L/16) (ViT-L/16) (ResNet152x4) (EfficientNet-L2)\nImageNet 88.55 \u00b1 0.04 87.76 \u00b1 0.03 85.30 \u00b1 0.02 87.54 \u00b1 0.02 88.4/88.5\n\u2217\nImageNet ReaL 90.72 \u00b1 0.05 90.54 \u00b1 0.03 88.62 \u00b1 0.05 90.54 90.55\nCIFAR-10 99.50 \u00b1 0.06 99.42 \u00b1 0.03 99.15 \u00b1 0.03 99.37 \u00b1 0.06 \u2212\nCIFAR-100 94.55 \u00b1 0.04 93.90 \u00b1 0.05 93.25 \u00b1 0.05 93.51 \u00b1 0.08 \u2212\nOxford-IIIT Pets 97.56 \u00b1 0.03 97.32 \u00b1 0.11 94.67 \u00b1 0.15 96.62 \u00b1 0.23 \u2212\nOxford Flowers-102 99.68 \u00b1 0.02 99.74 \u00b1 0.00 99.61 \u00b1 0.02 99.63 \u00b1 0.03 \u2212\nVTAB (19 tasks) 77.63 \u00b1 0.23 76.28 \u00b1 0.46 72.72 \u00b1 0.21 76.29 \u00b1 1.70 \u2212\nTPUv3-core-days 2.5k 0.68k 0.23k 9.9k 12.3k\nTable 2: Comparison with state of the art on popular image classification benchmarks. We report mean and standard deviation of the accuracies, averaged over three fine-tuning runs. Vision\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\nsmaller public ImageNet-21k dataset performs well too. \u2217Slightly improved 88.5% result reported\nin Touvron et al. (2020).\nVTAB (19 tasks)\n65\n70\n75\n80\nAccuracy [%]\nNatural (7 tasks)\n70\n80\n90\nSpecialized (4 tasks)\n80\n82\n85\n88\n90\nStructured (8 tasks)\n50\n60\n70 ViT-H/14 BiT-L (R152x4) VIVI-Ex-100% (R50x3) S4L (R50x1)\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\nmodel still took substantially less compute to pre-train than prior state of the art. However, we note\nthat pre-training efficiency may be affected not only by the architecture choice, but also other parameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days.\nFigure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\nmethods on this benchmark: BiT, VIVI \u2013 a ResNet co-trained on ImageNet and Youtube (Tschannen\net al., 2020), and S4L \u2013 supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the\nSpecialized the performance of the top two models is similar.\n4.3 PRE-TRAINING DATA REQUIREMENTS\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\nexperiments.\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT300M. To boost the performance on the smaller datasets, we optimize three basic regularization\nparameters \u2013 weight decay, dropout, and label smoothing. Figure 3 shows the results after finetuning to ImageNet (results on other datasets are shown in Table 5)2\n. When pre-trained on the\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\nwith JFT-300M, do we see the full benefit of larger models. Figure 3 also shows the performance\n2Note that the ImageNet pre-trained models are also fine-tuned, but again on ImageNet. This is because the\nresolution increase during fine-tuning improves the performance.\n6\nPublished as a conference paper at ICLR 2021\nImageNet ImageNet-21k JFT-300M\nPre-training dataset\n70\n75\n80\n85\n90\nImageNet Top1 Accuracy [%]\nBiT\nViT-B/32\nViT-B/16\nViT-L/32\nViT-L/16\nViT-H/14\nFigure 3: Transfer to ImageNet. While\nlarge ViT models perform worse than BiT\nResNets (shaded area) when pre-trained on\nsmall datasets, they shine when pre-trained on\nlarger datasets. Similarly, larger ViT variants\novertake smaller ones as the dataset grows.\n10 M 30 M 100 M 300 M\nNumber of JFT pre-training samples\n30\n40\n50\n60\n70\nLinear 5-shot ImageNet Top1 [%]\nViT-L/16\nViT-L/32\nViT-B/32\nViT-b/32\nResNet50x1 (BiT)\nResNet152x2 (BiT)\nFigure 4: Linear few-shot evaluation on ImageNet versus pre-training size. ResNets perform better with smaller pre-training datasets\nbut plateau sooner than ViT, which performs\nbetter with larger pre-training. ViT-b is ViT-B\nwith all hidden dimensions halved.\n10\n2 10\n3\n90\n95\nTransfer accuracy [%]\nAverage-5\nTransformer (ViT)\nResNet (BiT)\nHybrid\n10\n2 10\n3\n75\n80\n85\n90\nImageNet\nTransformer (ViT)\nResNet (BiT)\nHybrid\nTotal pre-training compute [exaFLOPs]\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\nResNets, and hybrids. Vision Transformers generally outperform ResNets with the same computational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap\nvanishes for larger models.\nregion spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but\nwith the larger datasets, ViT overtakes.\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT300M dataset. We do not perform additional regularization on the smaller subsets and use the same\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\nachieved during training. To save compute, we report few-shot linear accuracy instead of full finetuning accuracy. Figure 4 contains the results. Vision Transformers overfit more than ResNets with\ncomparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true\nfor ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\ndata is sufficient, even beneficial.\nOverall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\n(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT\nis an exciting direction of future work.\n7\nPublished as a conference paper at ICLR 2021\n4.4 SCALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models\u2019 performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pretrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\nbackbone).\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Appendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\nperformance/compute trade-off. ViT uses approximately 2 \u2212 4\u00d7 less compute to attain the same\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small computational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\n4.5 INSPECTING VISION TRANSFORMER\nInput Attention\nFigure 6: Representative examples of attention from the\noutput token to the input\nspace. See Appendix D.7 for\ndetails.\nTo begin to understand how the Vision Transformer processes image data, we analyze its internal representations. The first layer of\nthe Vision Transformer linearly projects the flattened patches into a\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top principal components of the the learned embedding filters. The components resemble plausible basis functions for a low-dimensional\nrepresentation of the fine structure within each patch.\nAfter the projection, a learned position embedding is added to the\npatch representations. Figure 7 (center) shows that the model learns\nto encode distance within the image in the similarity of position embeddings, i.e. closer patches tend to have more similar position embeddings. Further, the row-column structure appears; patches in the\nsame row/column have similar embeddings. Finally, a sinusoidal\nstructure is sometimes apparent for larger grids (Appendix D). That\nthe position embeddings learn to represent 2D image topology explains why hand-crafted 2D-aware embedding variants do not yield\nimprovements (Appendix D.4).\nSelf-attention allows ViT to integrate information across the entire\nimage even in the lowest layers. We investigate to what degree\nthe network makes use of this capability. Specifically, we compute\nthe average distance in image space across which information is\nintegrated, based on the attention weights (Figure 7, right). This\n\u201cattention distance\u201d is analogous to receptive field size in CNNs.\nWe find that some heads attend to most of the image already in the lowest layers, showing that\nthe ability to integrate information globally is indeed used by the model. Other attention heads\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we find that the model attends to image\nregions that are semantically relevant for classification (Figure 6).\n4.6 SELF-SUPERVISION\nTransformers show impressive performance on NLP tasks. However, much of their success stems\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\n8\nPublished as a conference paper at ICLR 2021\nRGB embedding filters\n(first 28 principal components)\n1 2 3 4 5 6 7\nInput patch column\n1\n2\n3\n4\n5\n6\n7\nInput patch row\nPosition embedding similarity\n1\n1\nCosine similarity\n0 5 10 15 20\nNetwork depth (layer)\n0\n20\n40\n60\n80\n100\n120\nMean attention distance (pixels)\nViT-L/16\nHead 1\nHead 2\nHead 3\n...\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Similarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position\nembedding of the patch with the indicated row and column and the position embeddings of all other\npatches. Right: Size of attended area by head and network depth. Each dot shows the mean attention\ndistance across images for one of 16 heads at one layer. See Appendix D.7 for details.\net al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\nsignificant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\net al., 2020b; He et al., 2020; Bachman et al., 2019; Henaff et al., 2020) to future work. \u00b4\n5 CONCLUSION\nWe have explored the direct application of Transformers to image recognition. Unlike prior works\nusing self-attention in computer vision, we do not introduce image-specific inductive biases into\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\nThus, Vision Transformer matches or exceeds the state of the art on many image classification\ndatasets, whilst being relatively cheap to pre-train.\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring selfsupervised pre-training methods. Our initial experiments show improvement from self-supervised\npre-training, but there is still large gap between self-supervised and large-scale supervised pretraining. Finally, further scaling of ViT would likely lead to improved performance.\nACKNOWLEDGEMENTS\nThe work was performed in Berlin, Zurich, and Amsterdam. We thank many colleagues at Google \u00a8\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the opensource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Luci\u02c7 c, Noam \u00b4\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\nREFERENCES\nSamira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In ACL, 2020.\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\nmutual information across views. In NeurIPS, 2019.\n9\nPublished as a conference paper at ICLR 2021\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\nICLR, 2019.\nI. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.\nIn ICCV, 2019.\nLucas Beyer, Olivier J. Henaff, Alexander Kolesnikov, Xiaohua Zhai, and A \u00b4 aron van den Oord. Are \u00a8\nwe done with imagenet? arXiv, 2020.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv, 2020.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\npixels. In ICML, 2020a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML, 2020b.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv, 2019.\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between selfattention and convolutional layers. In ICLR, 2020.\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL, 2019.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D\u2019Amour, Dan Moldovan, Sylvan\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convolutional neural networks. arXiv, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In CVPR, 2020.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. arXiv, 2019.\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\ndetection. In CVPR, 2018.\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\nIn ICCV, 2019.\nZilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and\nThomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020.\nOlivier J. Henaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, \u00b4\nand Aaron van den Oord. Data-efficient image recognition with contrastive predictive coding. In\nICML, 2020.\n10\nPublished as a conference paper at ICLR 2021\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. 2015.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.\nY. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1:541\u2013551, 1989.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. arXiv, 2020.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\nSimple and Performant Baseline for Vision and Language. In Arxiv, 2019.\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. arXiv, 2020.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised\npretraining. In ECCV, 2018.\nM. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In\nICVGIP, 2008.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR,\n2012.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In ICML, 2018.\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30(4):838\u2013855, 1992. doi: 10.1137/0330046. URL\nhttps://doi.org/10.1137/0330046.\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv\npreprint arXiv:1903.10520, 2019.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding with unsupervised learning. Technical Report, 2018.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. Technical Report, 2019.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. In NeurIPS, 2019.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, 2017.\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint\nmodel for video and language representation learning. In ICCV, 2019.\n11\nPublished as a conference paper at ICLR 2021\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy. In NeurIPS. 2019.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy: Fixefficientnet. arXiv preprint arXiv:2003.08237, 2020.\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain\nGelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\n2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a.\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. arXiv preprint\narXiv:2003.07853, 2020b.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\nLearning deep transformer models for machine translation. In ACL, 2019.\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nCVPR, 2018.\nDirk Weissenborn, Oscar Tackstr \u00a8 om, and Jakob Uszkoreit. Scaling autoregressive video models. In \u00a8\nICLR, 2019.\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\nfor computer vision. arxiv, 2020.\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student\nimproves imagenet classification. In CVPR, 2020.\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised SemiSupervised Learning. In ICCV, 2019a.\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\npreprint arXiv:1910.04867, 2019b.\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\nCVPR, 2020.\n12\nPublished as a conference paper at ICLR 2021\nModels Dataset Epochs Base LR LR decay Weight decay Dropout\nViT-B/{16,32} JFT-300M 7 8 \u00b7 10\u22124\nlinear 0.1 0.0\nViT-L/32 JFT-300M 7 6 \u00b7 10\u22124\nlinear 0.1 0.0\nViT-L/16 JFT-300M 7/14 4 \u00b7 10\u22124\nlinear 0.1 0.0\nViT-H/14 JFT-300M 14 3 \u00b7 10\u22124\nlinear 0.1 0.0\nR50x{1,2} JFT-300M 7 10\u22123\nlinear 0.1 0.0\nR101x1 JFT-300M 7 8 \u00b7 10\u22124\nlinear 0.1 0.0\nR152x{1,2} JFT-300M 7 6 \u00b7 10\u22124\nlinear 0.1 0.0\nR50+ViT-B/{16,32} JFT-300M 7 8 \u00b7 10\u22124\nlinear 0.1 0.0\nR50+ViT-L/32 JFT-300M 7 2 \u00b7 10\u22124\nlinear 0.1 0.0\nR50+ViT-L/16 JFT-300M 7/14 4 \u00b7 10\u22124\nlinear 0.1 0.0\nViT-B/{16,32} ImageNet-21k 90 10\u22123\nlinear 0.03 0.1\nViT-L/{16,32} ImageNet-21k 30/90 10\u22123\nlinear 0.03 0.1\nViT-\u2217 ImageNet 300 3 \u00b7 10\u22123\ncosine 0.3 0.1\nTable 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet we found it beneficial to additionally apply gradient\nclipping at global norm 1. Training resolution is 224.\nAPPENDIX\nA MULTIHEAD SELF-ATTENTION\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural architectures. For each element in an input sequence z \u2208 R\nN\u00d7D, we compute a weighted sum over all\nvalues v in the sequence. The attention weights Aij are based on the pairwise similarity between\ntwo elements of the sequence and their respective query q\ni\nand key k\nj\nrepresentations.\n[q, k, v] = zUqkv Uqkv \u2208 R\nD\u00d73Dh , (5)\nA = softmax \u0010\nqk>/\np\nDh\n\u0011\nA \u2208 R\nN\u00d7N , (6)\nSA(z) = Av . (7)\nMultihead self-attention (MSA) is an extension of SA in which we run k self-attention operations,\ncalled \u201cheads\u201d, in parallel, and project their concatenated outputs. To keep compute and number of\nparameters constant when changing k, Dh (Eq. 5) is typically set to D/k.\nMSA(z) = [SA1(z); SA2(z); \u00b7 \u00b7 \u00b7 ; SAk(z)] Umsa Umsa \u2208 R\nk\u00b7Dh\u00d7D (8)\nB EXPERIMENT DETAILS\nB.1 TRAINING\nTable 3 summarizes our training setups for our different models. We found strong regularization\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\ntraining is done on resolution 224.\nB.1.1 FINE-TUNING\nWe fine-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\nremaining data. For final results we train on the entire training set and evaluate on the respective\ntest data. For fine-tuning ResNets and hybrid models we use the exact same setup, with the only\nexception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,\n13\nPublished as a conference paper at ICLR 2021\nDataset Steps Base LR\nImageNet 20 000 {0.003, 0.01, 0.03, 0.06}\nCIFAR100 10 000 {0.001, 0.003, 0.01, 0.03}\nCIFAR10 10 000 {0.001, 0.003, 0.01, 0.03}\nOxford-IIIT Pets 500 {0.001, 0.003, 0.01, 0.03}\nOxford Flowers-102 500 {0.001, 0.003, 0.01, 0.03}\nVTAB (19 tasks) 2 500 0.01\nTable 4: Hyperparameters for fine-tuning. All models are fine-tuned with cosine learning rate decay,\na batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,\nfine-tuning resolution is 384.\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\nthis run and our sweep. Finally, if not mentioned otherwise, all fine-tuning experiments run at 384\nresolution (running fine-tuning at different resolution than training is common practice (Kolesnikov\net al., 2020)).\nWhen transferring ViT models to another dataset, we remove the whole head (two linear layers) and\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer.\nFor VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter\nsetting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this\nsetting by running a small sweep over two learning rates and two schedules, and selecting the setting\nwith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used\nin Kolesnikov et al. (2020), except that we do not use task-specific input resolutions. Instead we find\nthat Vision Transformer benefits most from a high resolution (384 \u00d7 384) for all tasks.\nB.1.2 SELF-SUPERVISION\nWe employ the masked patch prediction objective for preliminary self-supervision experiments. To\ndo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable\n[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is\n(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\npatch representations.\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\nuse Adam, with a base learning rate of 2 \u00b7 10\u22124\n, warmup of 10k steps and cosine learning rate decay.\nAs prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 \u00d7 4 downsized version of the 16 \u00d7 16\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\nwell, though L2 was slightly worse. We report final results only for option 1) because it has shown\nbest few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.\n(2019) but results were also slightly worse on our few-shot metrics.\nLastly, we would like to remark that our instantiation of masked patch prediction doesn\u2019t require\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to similar performance gains on ImageNet classification. That is, we observed diminishing returns on\ndownstream performance after 100k pretraining steps, and see similar gains when pretraining on\nImageNet.\nC ADDITIONAL RESULTS\nWe report detailed results corresponding to the figures presented in the paper. Table 5 corresponds\nto Figure 3 from the paper and shows transfer performance of different ViT models pre-trained\non datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to\n14\nPublished as a conference paper at ICLR 2021\nViT-B/16 ViT-B/32 ViT-L/16 ViT-L/32 ViT-H/14\nImageNet CIFAR-10 98.13 97.77 97.86 97.94 -\nCIFAR-100 87.13 86.31 86.35 87.07 -\nImageNet 77.91 73.38 76.53 71.16 -\nImageNet ReaL 83.57 79.56 82.19 77.83 -\nOxford Flowers-102 89.49 85.43 89.66 86.36 -\nOxford-IIIT-Pets 93.81 92.04 93.64 91.35 -\nImageNet-21k CIFAR-10 98.95 98.79 99.16 99.13 99.27\nCIFAR-100 91.67 91.97 93.44 93.04 93.82\nImageNet 83.97 81.28 85.15 80.99 85.13\nImageNet ReaL 88.35 86.63 88.40 85.65 88.70\nOxford Flowers-102 99.38 99.11 99.61 99.19 99.51\nOxford-IIIT-Pets 94.43 93.02 94.73 93.09 94.82\nJFT-300M CIFAR-10 99.00 98.61 99.38 99.19 99.50\nCIFAR-100 91.87 90.49 94.04 92.52 94.55\nImageNet 84.15 80.73 87.12 84.37 88.04\nImageNet ReaL 88.85 86.27 89.99 88.28 90.33\nOxford Flowers-102 99.56 99.27 99.56 99.45 99.68\nOxford-IIIT-Pets 95.80 93.40 97.11 95.83 97.56\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on ImageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\nare fine-tuned at 384 resolution. Note that the ImageNet results are computed without additional\ntechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.\nEpochs ImageNet ImageNet ReaL CIFAR-10 CIFAR-100 Pets Flowers exaFLOPs\nname\nViT-B/32 7 80.73 86.27 98.61 90.49 93.40 99.27 55\nViT-B/16 7 84.15 88.85 99.00 91.87 95.80 99.56 224\nViT-L/32 7 84.37 88.28 99.19 92.52 95.83 99.45 196\nViT-L/16 7 86.30 89.43 99.38 93.46 96.81 99.66 783\nViT-L/16 14 87.12 89.99 99.38 94.04 97.11 99.56 1567\nViT-H/14 14 88.08 90.36 99.50 94.71 97.11 99.71 4262\nResNet50x1 7 77.54 84.56 97.67 86.07 91.11 94.26 50\nResNet50x2 7 82.12 87.94 98.29 89.20 93.43 97.02 199\nResNet101x1 7 80.67 87.07 98.48 89.17 94.08 95.95 96\nResNet152x1 7 81.88 87.96 98.82 90.22 94.17 96.94 141\nResNet152x2 7 84.97 89.69 99.06 92.05 95.37 98.62 563\nResNet152x2 14 85.56 89.89 99.24 91.92 95.75 98.75 1126\nResNet200x3 14 87.22 90.15 99.34 93.53 96.32 99.04 3306\nR50x1+ViT-B/32 7 84.90 89.15 99.01 92.24 95.75 99.46 106\nR50x1+ViT-B/16 7 85.58 89.65 99.14 92.63 96.65 99.40 274\nR50x1+ViT-L/32 7 85.68 89.04 99.24 92.93 96.97 99.43 246\nR50x1+ViT-L/16 7 86.60 89.72 99.18 93.64 97.03 99.40 859\nR50x1+ViT-L/16 14 87.12 89.76 99.31 93.89 97.36 99.11 1668\nTable 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main\npaper. We show transfer accuracy on several datasets, as well as the pre-training compute (in exaFLOPs).\nFigure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of\nvarying size, as well as the estimated computational cost of their pre-training.\nD ADDITIONAL ANALYSES\nD.1 SGD VS. ADAM FOR RESNETS\nResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.\nHere we show the experiments that motivated this choice. Namely, we compare the fine-tuning\n15\nPublished as a conference paper at ICLR 2021\nResNet50 ResNet152x2\nDataset Adam SGD Adam SGD\nImageNet 77.54 78.24 84.97 84.37\nCIFAR10 97.67 97.46 99.06 99.07\nCIFAR100 86.07 85.17 92.05 91.06\nOxford-IIIT Pets 91.11 91.00 95.37 94.79\nOxford Flowers-102 94.26 92.06 98.62 99.32\nAverage 89.33 88.79 94.01 93.72\nTable 7: Fine-tuning ResNet models pre-trained with Adam and SGD.\n10\n0 10\n1\nRelative Compute\n0.2\n0.3\n0.4\n0.5\n0.6\nImageNet 5shot\nModels\nAll\nDepth\nPatch size\nWidth MLP\nWidth\n10\n0 10\n1\nRelative Compute\n0.4\n0.5\n0.6\n0.7\n0.8\nAverage 5shot\nModels\nAll\nDepth\nPatch size\nWidth MLP\nWidth\nFigure 8: Scaling different model dimensions of the Vision Transformer.\nperformance of two ResNets \u2013 50x1 and 152x2 \u2013 pre-trained on JFT with SGD and Adam. For\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\nin Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.\nThis justifies the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the\nabsolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only\nfor 7 epochs, not 30.\nD.2 TRANSFORMER SHAPE\nWe ran ablations on scaling different dimensions of the Transformer architecture to find out which\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\nfor different configurations. All configurations are based on a ViT model with 8 layers, D = 1024,\nDMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the network seems to result in the smallest changes. Decreasing the patch size and thus increasing the\neffective sequence length shows surprisingly robust improvements without introducing parameters.\nThese findings suggest that compute might be a better predictor of performance than the number of\nparameters, and that scaling should emphasize depth over width if any. Overall, we find that scaling\nall dimensions proportionally results in robust improvements.\nD.3 HEAD TYPE AND C L A S S TOKEN\nIn order to stay as close as possible to the original Transformer model, we made use of an additional\n[class] token, which is taken as image representation. The output of this token is then transformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\nin the single hidden layer.\nThis design is inherited from the Transformer model for text, and we use it throughout the main\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)\nthem, followed by a linear classifier\u2014just like ResNet\u2019s final feature map\u2014performed very poorly.\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead,\n16\nPublished as a conference paper at ICLR 2021\n0 1 2 3 4 5 6 7\nEpochs of training\n25\n30\n35\n40\n45\n50\n55\n60\nImageNet linear 5-shot accuracy [%]\nCLS-Token, lr=8e-4\nGAP, lr=8e-4\nGAP, lr=3e-4\nFigure 9: Comparison of class-token and global average pooling classifiers. Both work similarly\nwell, but require different learning-rates.\nPos. Emb. Default/Stem Every Layer Every Layer-Shared\nNo Pos. Emb. 0.61382 N/A N/A\n1-D Pos. Emb. 0.64206 0.63964 0.64292\n2-D Pos. Emb. 0.64001 0.64046 0.64022\nRel. Pos. Emb. 0.64032 N/A N/A\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\nImageNet 5-shot linear.\nthe difference in performance is fully explained by the requirement for a different learning-rate, see\nFigure 9.\nD.4 POSITIONAL EMBEDDING\nWe ran ablations on different ways of encoding spatial information using positional embedding. We\ntried the following cases:\n\u2022 Providing no positional information: Considering the inputs as a bag of patches.\n\u2022 1-dimensional positional embedding: Considering the inputs as a sequence of patches in\nthe raster order (default across all other experiments in this paper).\n\u2022 2-dimensional positional embedding: Considering the inputs as a grid of patches in two\ndimensions. In this case, two sets of embeddings are learned, each for one of the axes,\nX-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on\nthe path in the input, we concatenate the X and Y embedding to get the final positional\nembedding for that patch.\n\u2022 Relative positional embeddings: Considering the relative distance between patches to encode the spatial information as instead of their absolute position. To do so, we use 1-\ndimensional Relative Attention, in which we define the relative distance all possible pairs\nof patches. Thus, for every given pair (one as query, and the other as key/value in the attention mechanism), we have an offset pq \u2212 pk, where each offset is associated with an\nembedding. Then, we simply run extra attention, where we use the original query (the\ncontent of query), but use relative positional embeddings as keys. We then use the logits from the relative attention as a bias term and add it to the logits of the main attention\n(content-based attention) before applying the softmax.\nIn addition to different ways of encoding spatial information, we also tried different ways of incorporating this information in our model. For the 1-dimensional and 2-dimensional positional\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\n17\nPublished as a conference paper at ICLR 2021\n1 2 3 4 5 6 7 8 9 10 11 12 13 14\nInput patch column\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nInput patch row\nViT-L16\n7 epochs, LR=0.0002, WD=0.01\n1\n1\nCosine similarity\n1 2 3 4 5 6 7 8 9 10 11 12 13 14\nInput patch column\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nInput patch row\nViT-L16\n7 epochs, LR=0.0004, WD=0.1\n1\n1\nCosine similarity\n1 2 3 4 5 6 7 8 9 10 11 12 13 14\nInput patch column\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nInput patch row\nViT-L16\n14 epochs, LR=0.0004, WD=0.1\n1\n1\nCosine similarity\nFigure 10: Position embeddings of models trained with different hyperparameters.\nthe stem of them model and before feeding the inputs to the Transformer encoder (default across\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\neach layer (shared between layers).\nTable 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\nthere is a large gap between the performances of the model with no positional embedding and models with positional embedding, there is little to no difference between different ways of encoding\npositional information. We speculate that since our Transformer encoder operates on patch-level\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\npixel-level inputs, e.g., 14 \u00d7 14 as opposed to 224 \u00d7 224, and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies. Even so,\nthe specific pattern of position embedding similarity learned by the network depends on the training\nhyperparameters (Figure 10).\n0 5 10 15 20\nNetwork depth (layer)\n0\n20\n40\n60\n80\n100\n120\nMean attention distance (pixels)\nViT-L/16\nHead 1\nHead 2\nHead 3\n...\n0 5 10 15 20\nNetwork depth (layer)\n0\n20\n40\n60\n80\n100\n120\nR50x1 + ViT-L/16\nHead 1\nHead 2\nHead 3\n...\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\nheads at one layer. Image width is 224 pixels.\nD.5 EMPIRICAL COMPUTATIONAL COSTS\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\n18\nPublished as a conference paper at ICLR 2021\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\ndifference between inference and backprop speed is a constant model-independent factor.\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\nfor the largest models at the largest resolutions.\nAnother quantity of interest is the largest batch-size each model can fit onto a core, larger being\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\nThis shows that large ViT models have a clear advantage in terms of memory-efficiency over ResNet\nmodels.\n64 128 224 384 512\nInput size [px]\n10\n2\n10\n3\n10\n4\nPeak inference speed [img/sec/core]\n64 128 224 384 512\nInput size [px]\n10\n2\n10\n3\nLargest per-core batch-size\nR50x1\nR50x2\nViT-B/32\nViT-L/32\nViT-B/16\nViT-L/16\nViT-H/14\nR152x4\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size fitting on device with\nvarious architectures across input sizes. ViT models are clearly more memory-efficient.\nD.6 AXIAL ATTENTION\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run selfattention on large inputs that are organized as multidimensional tensors. The general idea of axial\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\ninstead of applying 1-dimensional attention to the flattened version of the input. In axial attention,\neach attention mixes information along a particular axis, while keeping information along the other\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\nall the convolutions with kernel size 3 \u00d7 3 in a ResNet50 are replaced by axial self-attention, i.e.\na row and column attention, augmented by relative positional encoding. We have implemented\nAxialResNet as a baseline model.3\n.\nMoreover, we have modified ViT to process inputs in the 2-dimensional shape, instead of a 1-\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\ncolumn-self-attention plus an MLP.\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on ImageNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\n3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\nunlocked by a carefully optimized implementation.\n19\nPublished as a conference paper at ICLR 2021\n10\n2\nTotal compute [exaFLOPs]\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\nImageNet 5-shot linear top-1 accuracy\nAxialViT-B/16\nAxialViT-B/32\nViT-B/16\nViT-B/32\nResNet50\nAxialResNet50\n10\n2 10\n3\nPeak inference speed [img/sec/core]\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\nImageNet 5-shot linear top-1 accuracy\nAxialViT-B/16\nAxialViT-B/32\nViT-B/16\nViT-B/32\nResNet50\nAxialResNet50\nFigure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet\n5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left).\nthe cost of more compute. This is because in Axial-ViT models, each Transformer block with global\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column selfattention and although the sequence length that self-attention operates on is smaller in axial case,\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\non TPUs (Figure 13, right).\nD.7 ATTENTION DISTANCE\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed\nthe average distance spanned by attention weights at different layers (Figure 11). This \u201cattention\ndistance\u201d is analogous to receptive field size in CNNs. Average attention distance is highly variable\nacross heads in lower layers, with some heads attending to much of the image, while others attend\nto small regions at or near the query location. As depth increases, attention distance increases for all\nheads. In the second half of the network, most heads attend widely across tokens.\nD.8 ATTENTION MAPS\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we\nused Attention Rollout (Abnar & Zuidema, 2020). Briefly, we averaged attention weights of ViTL/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts\nfor the mixing of attention across tokens through all layers.\nD.9 OBJECTNET RESULTS\nWe also evaluate our flagship ViT-H/14 model on the ObjectNet benchmark following the evaluation\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\nD.10 VTAB BREAKDOWN\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\n20\nPublished as a conference paper at ICLR 2021\n1 2 3 4 5 6 7 8\n9 10 11 12 13 14 15 16\n17 18 19 20 21 22 23 24\n25 26 27 28 29 30 31 32\n33 34 35 36 37 38 39 40\n41 42 43 44 45 46 47 48\n49 50 51 52 53 54 55 56\n57 58 59 60 61 62 63 64\n65 66 67 68 69 70 71 72\n73 74 75 76 77 78 79 80\n81 82 83 84 85 86 87 88\n89 90 91 92 93 94 95 96\n97 98 99 100 101 102 103 104\n105 106 107 108 109 110 111 112\n113 114 115 116 117 118 119 120\n121 122 123 124 125 126 127 128\nFigure 14: Further example attention maps as in Figure 6 (random selection).\n21\nPublished as a conference paper at ICLR 2021\nTable 9: Breakdown of VTAB-1k performance across tasks.\nCaltech101\nCIFAR-100\nDTD\nFlowers102\nPets\nSun397\nSVHN\nCamelyon\nEuroSAT\nResisc45\nRetinopathy\nClevr-Count\nClevr-Dist\nDMLab\ndSpr-Loc\ndSpr-Ori\nKITTI-Dist\nsNORB-Azim\nsNORB-Elev\nMean\nViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6\nViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7\n22"
  },
  {
    "id": 7,
    "name": "inaugural_art_week_riyadh.txt",
    "content": "The inaugural Art Week Riyadh (AWR), organised by Saudi Arabia's Visual Arts Commission (VAC) and held between 6-13 April, was, according to its website, \"a non-commercial\" curated exhibition of galleries; explicitly not a fair.\n\nAnd yet, from the feel to the physical layout\u2014its central exhibition saw 32 galleries take stands inside a central hall in the Jax district, a creative hub in Diriyah, northwest of the city\u2014the event not only resembled an art fair, but largely functioned like one too. Dealers were encouraged to only bring available work which was ideally \u201cmuseum-grade\u201d, as one participant told The Art Newspaper. Price lists were seen on stands; sales were conducted during the event's run.\n\nTitled At The Edge and organised by AWR's artistic director Vittoria Matarrese with associate curators Basma Harasani and Victoria Gandit Lelandais, the main exhibition featured galleries mostly drawn from Saudi Arabia and the wider region, spread across two halls. Standouts included a spare Ayesha Sultana cityscape at Experimenter, Mohamed Bourouissa\u2019s installation of photos printed on compacted car body parts at Mennour, and two Mohammed Al Resayes canvases from 1985 at Hewar. A quad of Miramar Al Nayyar paintings at Tabari Artspace were mesmeric in their asemic, ribbon worm-like forms and otherworldly glow.\n\nA moving image section presented films from ten artists, including Bani Abidi, Zineb Sedira and Theaster Gates, while Collections in Dialogue presented selections from three Saudi collections: Art Jameel, Ithra, and the SRMG (Saudi Research and Media Group).\n\n\nVisitors at the Al Mousa Center\n\nCourtesy of Visual Arts Commission, Saudi Arabia\n\nThe satellite programme resembled that of a usual art week: an impressive lineup of open studios, numerous gallery shows, of which Basmah Felemban at Athr was a highlight, as well as organised tours geared towards introducing the international visitor to Riyadh and Saudi\u2019s art ecologies. Especially commendable was the inclusion of Al Mousa Center in the programme, a mall of mostly Modern-focused galleries that have been largely sidelined in the new Saudi art scene.\n\nThe fair's organisers maintain the stance that AWR was not a commercial venture. \u201cIt's too long that people take galleries as vendors. For me, the work of a gallery, it's not to be a vendor \u2026 I wasn't looking for vendors,\u201d said AWR's artistic director Vittoria Matarese. She adds that the scale of work in some presentations, as well as the installations placed between booths\u2014including Kader Attia\u2019s carpet of broken mirrors and a new Mohammad Alfaraj commission of palms sprouting from the walls and floors\u2014further distinguish the exhibition from a fair.\n\nThe future of fairs?\nYet many exhibitors noted the hybrid feel of AWR, which boasted high production costs and included some less commercial work, as being halfway between an institutional show and a fair. \u201cThese events are interesting for educating the audience. It feels like a biennial, which they are already more or less accustomed to because they had two editions of the Diriyah Biennale, but it also has a feel of an art fair,\u201d said Jal Hamad, a director at the Madrid-based Sabrina Amrani gallery, pointing out that this hybrid model is much less intimidating for new collectors who might not be familiar with how to deal with galleries or research artists.\n\nMohammed Hafiz of Athr is meanwhile enthusiastic about the sharp uptick in young Saudi collectors thanks to the Ministry of Culture\u2019s efforts, a far cry from when his gallery launched 17 years ago. He explains: \u201cI think the older collectors have already decided if they're going to be collectors or not, and they've decided what to collect. Is it Persian carpets, Islamic antiquities, or art? But the younger ones are still figuring out what to do. When you come to the Art Week, you see your peers there. \u2018What did you buy? I bought this. Why don't you look at this?\u2019 It just creates this dynamic.\u201d\n\nIt\u2019s a marked shift from the older tendency that VAC's chief executive Dina Amin points to: \u201cUnlike, elsewhere in the world, where you have a very public engagement with collecting, cultural norms here often mean that people are very private.\u201d\n\nFor dealers, there was less freedom than found at a usual fair: curators selected artists from each gallery roster to correspond to the exhibition's themes. There were far fewer risks too: dealers didn\u2019t pay for space, production or shipping, only their flights and hotels if they wanted to attend; most did. Why wouldn\u2019t they? Here was a rare chance to test out a new market, meet regional artists, curators, and art world power players, and answer the implicit questions: does Saudi\u2019s historically weak commercial sector show promise? Can it hold its own against the regional juggernauts of Art Dubai, which was held just a week later and Art Abu Dhabi? With dealers reporting sales to individual Saudi collectors new to their programme and (as of the preview days) strong interest from Saudi institutions, the answers seem to be a resounding yes and yes.\n\n\u201cI like it, to be honest. Maybe this is the future of fairs,\u201d said Sunny Rahbar, founder of The Third Line gallery in Dubai. \u201cLess pressure on the galleries and the collectors, knowing that everything is for sale, but not feeling like you need to stay in the booth.\u201d Hafiz agrees: \u201cIf you don't interact around art, then the conversations are not happening, the interest is not generated, the transactions don't happen. I think the art week idea is a very important one. It's the first commercial-centric initiative by the Visual Arts Commission.\u201d\n\nCommercial legislation issues\nSo why the insistence that this is non-commercial? A key reason is that the works were imported on a temporary permit through the non-commercial Ministry of Culture, under which the VAC sits. What this means is that anything sold by non-Saudi galleries essentially has to be exported back to their home countries before being re-imported.\n\nSaudi Arabia presently does not have any art-specific exemptions for imports either, so fees amount to a sizeable 20%\u20145% duty on fine art plus 15% VAT, though for dealers within the GCC (Gulf Cooperation Council) customs union\u2014which is most of the participating international galleries\u2014VAT drops to 5% if their annual export value is under around $100,000. While things move fast in KSA, federal legislation, as elsewhere, moves slowly. Amin notes that the VAC has recently launched a series of licenses that make setting up a gallery or organising an exhibition as a non-KSA entity much easier, while cautioning that \u201cour objective is the cultural engagement\u201d. And as for what galleries might do, \u201cthat's their business, how they do their business, but it's not our business\u201d.\n\nArt Week Riyadh also provided an opportunity to connect with young, roving galleries mushrooming around the Gulf, like Bawa, Thaer Select and the participating Hunna. The Bahrain and Lisbon-based founder of Forat Gallery, focusing on unsung female Modernists from the Gulf, had launched just two days prior, and one Los Angeles-based collector was so taken with the dynamism of the Saudi art scene that he recently moved to Riyadh.\n\nIt remains to be seen whether AWR will embrace, or at least acknowledge, its commercial nature in future editions. Though perhaps that is a moot point. The line between the market and institutional sectors of the art world has never been so thin or faintly dotted. Galleries are regularly tasked with supporting production at museum shows and biennials, and even grande dames like Venice are among the most important marketplaces for art today: \u201cthe world\u2019s best art fair,\u201d as the collector Alain Servais once memorably dubbed it. And yet\u2014a fair is a fair is a fair, isn\u2019t it?\n\nSaudi Arabia\nRiyadh\nArt market"
  },
  {
    "id": 8,
    "name": "money_markets.txt",
    "content": "Table of Contents\nWhat Is the Money Market?\nHow It Works\nWho Can Invest?\nThe One-Buck Baseline\nTypes of Instruments\nMoney Markets vs. Capital Markets\nPros and Cons\nMoney Market FAQs\nThe Bottom Line\nBonds Fixed Income\nMoney Markets: What They Are, How They Work, and Who Uses Them\nBy Adam Hayes Updated July 04, 2024\nReviewed by Michael J Boyle\nFact checked by Yarilet Perez\nDefinition\nThe money market involves the purchase and sale of large volumes of very short-term debt products, such as overnight reserves or commercial paper.\nWhat Is the Money Market?\nThe money market refers to trading in very short-term debt investments. It involves continuous large-volume trades between institutions and traders at the wholesale level. It includes money market mutual funds bought by individual investors and money market accounts opened at banks at the retail level.\n\nThe money market is characterized by a high degree of safety and relatively low rates of return on investment.\n\nKey Takeaways\nThe money market involves the purchase and sale of large volumes of very short-term debt products such as overnight reserves or commercial paper.\nAn individual can invest in the money market by purchasing a money market mutual fund, buying a Treasury bill, or by opening a money market account at a bank.\nMoney market investments are characterized by safety and liquidity with money market fund shares targeted at $1.\nMoney market accounts offer higher interest rates than normal savings accounts but they have higher account minimums and limits on withdrawals.\nUnderstanding the Money Market\nThe money market is one of the pillars of the global financial system. It involves overnight swaps of vast amounts of money between banks and the U.S. government. The majority of money market transactions are wholesale transactions that take place between financial institutions and companies.\n\nInstitutions that participate in the money market include banks that lend to each other and to large companies in the euro currency and time deposit markets. They also include companies that raise money by selling commercial paper into the market and investors who purchase bank CDs as a safe place to park money in the short term.\n\nFast Fact\nSome of those wholesale transactions eventually make their way into the hands of consumers as components of money market mutual funds and other investments.\n\nWho Can Invest in the Money Market?\nIndividuals can invest in the money market by buying money market funds, short-term certificates of deposit (CDs), municipal notes, or U.S. Treasury bills. The money market has retail locations for individual investors. They include local banks, the U.S. government's TreasuryDirect website, and brokerages.\n\nTake the Next Step to Invest\nAdvertiser Disclosure\n\n\n\nThe One-Buck Baseline\nMoney market funds seek stability and security with the goal of never losing money and keeping net asset value (NAV) at $1. This one-buck NAV baseline gave rise to the phrase \"break the buck.\" Some of the original investment is gone and investors will lose money if the value falls below the $1 level.\n\nThis scenario happens very rarely, however. It last occurred in 2008 and involved a fund that held assets of the bankrupt Lehman Brothers investment company. Its investors eventually received 98 cents on the dollar.\n1\n\nMany money market funds aren't FDIC-insured so they can nonetheless lose money.\n2\n\nTypes of Money Market Instruments\nMoney Market Funds\nThe wholesale money market is limited to companies and financial institutions that lend and borrow in amounts ranging from $5 million to well over $1 billion per transaction. Mutual funds offer baskets of these products to individual investors. The net asset value (NAV) of such funds is intended to stay at $1.\n\nMoney Market Accounts \nMoney market accounts are a type of savings account. They pay slightly higher interest rates than regular savings accounts but they often come with restrictions on withdrawing money or writing checks. Withdrawals are limited by federal regulations. The bank will promptly convert the money market account to a checking account if the limits are exceeded.\n\nBanks typically calculate interest on a money market account daily and make a monthly credit to the account.\n\nAverage interest rates for money market accounts can vary based on the amount deposited. The best-paying money market account advertised online as of July 2024 was offered by Brilliant Bank at 5.35% with a $1,000 minimum deposit.\n3\n\nMoney market accounts have become more popular because of their perceived safety compared to more volatile investments given a high interest rate market.\n\nImportant\nFunds in money market accounts are insured by the Federal Deposit Insurance Corporation (FDIC) when they're held at banks and the National Credit Union Administration (NCUA) when they're held in credit unions.\n2\n4\n \n\nCertificates of Deposit (CDs)\nMost certificates of deposit (CDs) aren't strictly money market funds because they're sold with terms of up to 10 years. CDs with terms as short as three months to six months are available, however.\n\nLarger deposits and longer terms yield better interest rates just as they do with money market accounts. Rates in early July 2024 ranged from about 5.35% to 6.00%.\n\nThe rates offered on a CD remain constant for the deposit period, unlike with a money market account. There's usually a penalty associated with an early withdrawal of funds from a CD. They've gained in popularity due to their safety and the relatively high rates available, however.\n\nU.S. Treasury Bills\nThe U.S. government issues Treasury bills in the money market with maturities ranging from a few days to one year. Cash management bills come with maturities of a few days to one year.\n5\n6\n\nPrimary dealers buy these bills in large amounts directly from the government to trade between themselves or to sell to individual investors. Individual investors can buy them directly from the government through the TreasuryDirect website or a bank or a broker. State, county, and municipal governments also issue short-term notes.\n\nCommercial Paper\nThe commercial paper market is for buying and selling unsecured loans for corporations in need of a short-term cash infusion. Only highly creditworthy companies participate in this market so the risks remain low.\n\nCommercial paper is a popular borrowing mechanism in the wholesale market because the interest rates are higher than for bank time deposits or Treasury bills. A greater range of maturities is available as well, averaging about 30 days and extending up to nine months.\n7\n The risk of default is significantly higher for commercial paper than for bank or government instruments, however.\n\nBanker's Acceptances\nA banker's acceptance is a short-term loan that's guaranteed by a bank. Used extensively in foreign trade, a banker's acceptance is like a post-dated check. It serves as a guarantee that an importer who has ordered goods can pay for them.\n\nThere's a secondary market for buying and selling banker's acceptances at a discount.\n\nEurodollars\nEurodollars are dollar-denominated deposits held in foreign banks so they're not subject to Federal Reserve regulations. Very large deposits of eurodollars are held in banks in the Cayman Islands and the Bahamas.\n\nMoney market funds, foreign banks, and large corporations invest in them because they pay a slightly higher interest rate than U.S. government debt.\n\nRepos\nThe repo or repurchase agreement is part of the overnight lending money market. Treasury bills or other government securities are sold to another party with an agreement to repurchase them at a set price on a set date.\n\nMoney Markets vs. Capital Markets\nThe money market is defined as dealing in debt of less than one year. It's used primarily by governments and corporations to keep their cash flows steady and by investors to make a modest profit.\n\nThe capital market is dedicated to the sale and purchase of long-term debt and equity instruments.\n\nThe term \"capital markets\" refers to the entirety of the stock and bond markets. Stocks have no expiration date unless the company itself ceases to operate, unlike many money market products,\n\nAdvantages and Disadvantages of Money Markets\nMost money market securities are considered extremely low-risk due to the protection of FDIC insurance, backing by a government or bank, or the high creditworthiness of the borrowers. They're also very liquid. They can readily be exchanged for cash at short notice.\n\nThe tradeoff is that these investments have low returns. Money markets generally underperform other asset classes and often don't even keep pace with inflation. Any fees associated with an account can easily eat into these slim returns. And these advantages don't extend to all money market securities. Some aren't FDIC insured and there's a chance that even the most trustworthy borrowers may default.\n\nSome money market accounts have minimum balance requirements or restrictions on withdrawals.\n\nPros and Cons of Money Market Accounts\nPros\nExtremely low risk\n\nMay be insured by FDIC\n\nHighly liquid\n\nHigher returns than most bank accounts\n\nCons\nLow returns that may not keep pace with inflation\n\nNot all money market securities are insured\n\nMay have high minimum investments or withdrawal restrictions\n\nWhy Is It Called the Money Market?\nThe money market deals in highly liquid, very safe, short-term debt securities and these attributes make them virtual cash equivalents. They can be exchanged for cash at short notice.\n\nWhy Is the Money Market Important?\nThe money market keeps the financial economy running smoothly. It allows savers to lend money to those in need of short-term loans and it allocates capital toward its most productive use.\n\nThese loans are often made overnight or for a matter of days or weeks. They're needed by governments, corporations, and banks to meet their near-term obligations or regulatory requirements. And they allow those with some excess cash on hand to earn a small amount of interest.\n\nWhat Are Some Examples of Money Market Instruments?\nThe money market is composed of several types of securities including short-term Treasuries (T-bills), certificates of deposit (CDs), commercial paper, repurchase agreements (repos), and money market mutual funds that invest in these instruments. The money market funds typically have shares priced at $1.\n\nCan You Lose Money in the Money Market?\nMost money market accounts are insured by the FDIC up to $250,000 per institution, just like bank deposits. There's virtually no chance you'll lose your money by owning a CD or T-bill because money market instruments are very low risk.\n\nSome money market funds can \"break the buck\" and briefly incur losses during periods of extreme financial stress such as at the height of the 2008 financial crisis. This was quickly corrected, however.\n\nWhat Are the Downsides of Money Markets?\nMoney market investments pay very low returns because they're virtually risk-free. They can't provide substantial capital gains or investment growth compared to riskier assets like stocks or even bonds.\n\nSome types of money market accounts like CDs lock your money up until a future date that can be months or even years ahead.\n\nIndividual investors should also look carefully at the fees they'll be charged for money market accounts. They can eat into the already modest rates of return on these investments.\n\nThe Bottom Line\nMoney market accounts and money market funds are among the safest ways to invest money. They also have much lower returns than other investments and they may even fail to keep up with inflation.\n\nMany individuals and businesses use money markets as a short-term investment for their cash reserves. These investments are virtually risk-free and offer at least a modest return on savings."
  },
  {
    "id": 9,
    "name": "neuroplasticity_super_protocol.txt",
    "content": "Teach & Learn Better With A \"Neuroplasticity Super Protocol\"\n\nThank you for joining the Huberman Lab Podcast Neural Network\u2014a once a month newsletter with science and science-related tools for everyday life.\n\nFor this newsletter, I want to provide you some actionable information in condensed form. It relates to a talk I recently gave (hosted by Logitech) for teachers, and students of all ages.\n\nThere were two goals of the lecture:\n\nProvide an overview of the major discoveries on neuroplasticity and learning.\nShare a \u201cNeuroplasticity Super-Protocol\u201d based on those discoveries, so that anyone can teach and learn anything more efficiently.\nNote: This version of the \u201cNeuroplasticity Super-Protocol\u201d focuses on behavioral tools. If you want a description of the specific scientific references that support the steps listed below, please watch this video.\n\nNEUROPLASTICITY SUPER-PROTOCOL\n1. GET ALERT\nWe must be alert to trigger neuroplasticity (later, sleep completes the neuroplasticity/learning process). Getting alert involves many mechanisms but mainly the release of epinephrine (adrenaline) in the brain and body. One simple way to become more alert is 25-30 deep breaths (inhales through the nose, and exhales through the mouth). Then exhale your air and hold your breath with lungs empty for 15-60 seconds. Then inhale once and hold your breath. But don\u2019t force the breath hold; start to breathe normally immediately once you feel the impulse to breathe. Whether you rely on caffeine or not (I certainly do in the early portion of the day), try this prior to a learning bout.\n\n2. GET FOCUSED\nMental focus follows visual focus. To increase your level of focus on the task you are about to do, stare at a point on a wall or screen, or object for 30-60 seconds before starting (You can blink as needed). You\u2019ll be surprised how this takes a bit of effort\u2014that \u2018effort\u2019 you feel is \u201ctop-down\u201d attentional engagement and reflects the activity of neural circuits involving acetylcholine release in the brain, and other mechanisms too of course. Then move into the task at hand. Expect your mental focus to flicker on and off, especially at the start of a work/learning bout. [Obviously, having your phone off and out of the room and web browsers closed or limited to essential tabs only (or even better, the internet turned off) can help.]\n\n3. GENERATE REPETITIONS\nPerform the maximum number of repetitions you safely can in a given learning bout. For some types of learning, \u201crepetitions\u201d will be actual repeats of something- learning scales of music, for instance. We progress linearly for other types of learning by repeating the same process, such as reading or doing math problems. Regardless, the same principle holds; work to repeat the process a bit faster than is reflexive for you. This helps the mind from drifting off task and naturally keeps you alert. Will you make errors? Of course, which leads to #4.\n\n4. EXPECT & EMBRACE ERRORS\nProvided they don\u2019t comprise safety, errors during learning are terrific because they increase activation of the neural circuits that increase alertness. It makes sense, right? If you perform something correctly, why should your brain take notice? When we make errors, it feels \u201cstressful,\u201d but that is just an increase in attention that puts us in a much better place to perform and execute learning-related behaviors the next trial\u2014meaning on the next attempt. Computational modeling data suggests that an error rate of ~15% may be optimal and can help determine how difficult we should make a task. But don\u2019t worry too much about those specifics. Instead, keep doing repetitions and when you mess up, capitalize on it by doing another attempt (and another) while your forebrain is in that maximally attentive state.\n\n5. INSERT MICRO-REST INTERVALS (AT RANDOM)\nThis is a non-obvious way to increase repetitions and learn faster. Studies (in humans) have shown that when we are trying to learn something, if we pause every so often for 10seconds and do nothing during the pause, neurons in the hippocampus and cortex\u2014areas of the brain involved in learning and memory, engage the same patterns of neural activity that occurred during the actual activity of reading, musical practice, skill training, etc. but 10X faster\u2014meaning you get 10X neural repetitions completed during the pause. These \u201cgap-effects\u201d are similar to what happens in a deep sleep. The takeaway: randomly introduce 10 second pauses during learning. \u201cHow often?\u201d I get asked. A ratio of approximately 1 pause per every 2 minutes of learning is good but remember, distributed at random, so not every 2 minutes on the minute.\n\n6. USE RANDOM INTERMITTENT REWARD\nThe neural circuits that control rewards (all of which are brain chemical rewards, by the way) are closely tethered to the circuits that control motivation and the desire to pursue things, including learning. The question of how often to reward ourselves or others in order to keep motivation high is simple: make it random and intermittent. This is what casinos do to keep people gambling. It works. Predictable rewards lose their motivational impact quickly.\n\n7. LIMIT LEARNING SESSIONS TO 90 MINUTES\nSolid research shows that 90 minutes is about the longest period we can expect to maintain intense focus and effort toward learning. Shorter bouts are fine but after ~90 minutes, take a break (see #8). Also, space intense learning bouts 2-3 (or more) hours apart. Most people can\u2019t do more than 270 minutes of intense learning bouts per day.\n\n8. AFTER A LEARNING BOUT, DO A NSDR (NON-SLEEP DEEP REST) PROTOCOL\nTwo studies (on humans) published in the last 2 years show that shallow naps and/or NSDR can enhance the rate and depth of learning. This is an easy practice to incorporate. Within 1 hour of completing a learning bout, do a short NSDR protocol. You have options as to what NSDR you choose: Reveri is a zero-cost (research tested), self-hypnosis app, or take a brief 20 minute nap, or listen to an NSDR script such as Yoga Nidra (I like this 10 minute one and do it daily, or here is a longer 30 minute video that is excellent).\n\n9. GET QUALITY & SUFFICIENTLY LONG DEEP SLEEP THAT NIGHT (& THE NEXT, & THE NEXT\u2026)\nThe actual rewiring of neural circuits that underlies learning occurs during sleep and NSDR. Think of the learning bout as the \u201ctrigger\u201d or stimulus for the possibility that we might learn, but sleep and NSDR are when the actual learning- the neural circuit rewiring, occurs. I did an entire episode (4 actually) of the Huberman Lab Podcast on mastering sleep. I provided a summary of key points in Neural Network Newsletter #1. Our goal should be to get sleep right at least 80% of the time\u2014it takes some work to get there but it is well worth it.\n\nFINAL NOTES\nIn the future, I will talk about the pharmacology of accelerated/deeper learning but remember that behavioral protocols like the ones listed here are necessary no matter what. You don\u2019t have to do all 9 every learning session (although numbers 1, 2, 9 are non-negotiable).\n\nI\u2019ll be posting more on tools for neuroplasticity in the near future.\n\nNew episodes of The Huberman Lab Podcast are out each Monday on YouTube, Apple Podcasts, Spotify and other major podcast platforms. Please subscribe to those channels. We also launched a clips channel, where we\u2019ll post short segments from the episodes. I post additional science and science-based tools on Instagram and Twitter.\n\nThank you for your interest in science,\nAndrew"
  },
  {
    "id": 10,
    "name": "nfts_afterlife_aura.txt",
    "content": "NFTs: The Afterlife of the Aura\nJulia Friedman  David Hawkes \n\nTwo recent controversies suggest that the emergence of crypto art may forever undermine the hegemony of the object-centered art market. In March 2021, a firm known as Injective Protocol bought a Banksy print for $95,000, sold an NFT of it for $380,000, and publicly burned the original on YouTube. Then, in April 2021, a firm known as Daystrom attempted to auction off an NFT of a drawing by Jean-Michel Basquiat on the understanding that the purchaser would have the right to \u201cdeconstruct\u201d the original.\n\nBoth were provocations in the venerable tradition of Dada and Punk, and the pearl-clutching public reaction was an integrated response. Headlines included \u201cNFT: No Fucking Thanks,\u201d \u201cSickos\u201d and the withering deadpan of the BBC: \u201cBanksy Art Burned, Destroyed and Sold as Token in \u2018Money-making Stunt.\u2019\u201d\n\nThe idea that the destruction of art can be part of art is old news, having been espoused throughout the twentieth century by artists ranging from Yves Klein to Pete Townshend. Such auto-destruction aimed to make a grander statement than anything available within the formal confines of material art. In 1953, attempting \u201cto figure out a way to bring drawing into all whites,\u201d the young Robert Rauschenberg came up with the idea of erasing an extant drawing. Not one of his own, though. Rauschenberg was then pretty much unknown, and he insisted that the drawing erased had to be \u201creal art.\u201d He approached the abstract expressionist Willem de Kooning, whose work was held in the highest esteem. The chasm between the two men\u2019s artistic standing was the key to the project, pointedly framing the affair as a newcomer\u2019s challenge for dominance.\n\nThe \u201cErased de Kooning Drawing\u201d sent shock waves through the New York art world. It was simultaneously hailed as a daring act of Neo-Dada defiance and damned as ignorant vandalism. But Rauschenberg\u2019s stunt was an attempt to force his way into the artistic canon, not a challenge to the existence of the canon itself. Long after his apparently anti-aesthetic gesture had been comfortably assimilated into high art, Rauschenberg described it as \u201cpoetry.\u201d The advent of digital art NFTs is very different. A moral gulf separates an artist who painstakingly erases another\u2019s drawing, with his explicit permission, from the wanton destruction of an artist\u2019s work by the owner of its digital avatar.\n\n\nRobert Rauschenberg, Erased de Kooning Drawing, 1953. Traces of ink and crayon on paper, with mat and hand-lettered label in ink, in gold-leafed frame, 25 1/4 x 21 3/4 x 1/2 inches (64.1 x 55.2 x 1.3 cm). San Francisco Museum of Modern Art, Purchase through a gift of Phyllis Wattis\nThe NFT that represents the artwork stands in an antithetical, hostile relation to the original. The putative purchaser of the Basquiat non-fungible token was granted the option to \u201cdeconstruct\u201d the original, because by doing so, they would transform the NFT itself into the original. But even if Basquiat\u2019s handiwork had been destroyed, its reproductions would remain. In the twenty-first century, the NFT\u2014a digital imprint of the work in the blockchain\u2014is thus actually more unique than the original drawing itself. As BurntBanksy put it:\n\nIf you were to have the NFT and the physical piece, the value would be primarily in the physical piece. By removing the physical piece from existence and only having the NFT, we can ensure that the NFT, due to the smart contract ability of the blockchain, will ensure that no one can alter the piece and it is the true piece that exists in the world. By doing this, the value of the physical piece will then be moved onto the NFT.\n\nThe financial value of the artwork rises, Phoenix-like, from the ashes of the original\u2019s destruction and finds a new abode in the NFT. What was really destroyed when the Banksy was burned? Neither the image itself, which continues to exist online, nor access to the image, which is available to anyone with a computer or a smartphone. By physically destroying the Banksy print, the purchasers of the NFT attacked the Benjaminian \u201caura\u201d that dwelt within the original work of art.\n\n\nJean-Michel Basquiat, Free Comb with Pagoda (1986). Image: DaystromNFT\nThe \u201caura\u201d is what makes the experience of viewing Da Vinci\u2019s Mona Lisa in the Louvre, or his drawings at the Met, different from looking at their images in a book. It is inseparable from the viewer\u2019s visceral reaction to the physical traits of the work: variable pressure of the crayon on paper, the thickness of impasto brushstrokes or their glossy translucency, the weave of the canvas showing through the loosely applied imprimatura, the mutable effects of light playing on the surface at different times of day. An artwork\u2019s aura is also the source of its financial value, the reason the original Mona Lisa is worth more than a reproduction. But if the original is destroyed, there is nowhere physical for the aura to reside. The aura\u2019s abstract, symbolic nature is then revealed, and it becomes possible to package, market and sell the aura in the absence of the original. The destruction of the original allows the NFT to monetize the aura, imposing on it the form of financial value. As Daystrom explain:\n\nValue has become increasingly fungible, diluted and unstable in our evolving metaverse and there\u2019s a tremendous spike in user demand for exclusivity. NFT assets provide this exclusivity and create an entirely new online value system that was previously unimaginable.\n\nBut an \u201caura\u201d is not a material thing. Does it necessarily perish along with its physical incarnation? Perhaps it was not destroyed so much as transubstantiated, reborn into a financialized afterlife where it is no longer subject to mortal decay. Like BurntBanksy, Daystrom make a plea for authenticity, not a protest against it. But authenticity is no longer a quality of the original artwork, contingent on the artist\u2019s touch or painterly gesture. Authenticity is now a quality of the NFT that represents the original, and the only authenticity available today is statistical uniqueness. Yet people remain sentimentally attached to the old distinctions between authenticity and image, original and reproduction, reality and representation. The cries of fear and loathing at the prospect of destroying a Basquiat drawing (albeit not a great one) or a Banksy print (albeit one of an edition of 500) are not na\u00efve defenses of the artwork\u2019s lost integrity. They are inarticulate but nonetheless passionate protests against the postmodern condition. No wonder the word \u201cdeconstruction\u201d where simple \u201cdestruction\u201d would have sufficed was so triggering.\n\n\nOpenSea lot listing for BurntBanksy\nOf course, the owner of anything has always had the right to destroy it: that is what ownership means. There was a persistent rumor that Van Gogh\u2019s Portrait of Dr. Gachet was cremated along with its deceased owner in 1996. It turned out to be false, but the idea of an owner wantonly destroying the work evidently resonated with the Zeitgeist. To preempt any such plans, the federal government passed the Visual Artists Rights Act of 1990 to protect works of \u201crecognized stature\u201d from destruction by their owners. In this context, the new form of ownership represented by NFTs is arguably more democratic than its predecessor. When ownership involved taking physical possession of the unique artwork, the owner could easily prevent the public from viewing it by keeping it in a bank vault. In the digital age, the detachment of the aura from the artwork makes such hogging impossible, so perhaps this detachment is not the problem. The problem is the NFT\u2019s inherent antipathy towards the original.\n\nThat has the potential to become a very serious problem. NFTs are liable to physically attack the artworks they represent as long as there is a financial incentive for them to do so\u2014and such an incentive is hard-wired into their blockchain nature. In dystopian theory, NFTs could obliterate all actually existing works of art, replacing them with tokens of their financial value. The process would be seamless. NFTs simultaneously embody two kinds of abstraction: financial value and the aesthetic aura. The fact that both of these abstractions can be incarnated in the same symbol at the same time shows that they have become functionally identical. And if identical, then interchangeable.\n\nIf art is money, then money is art. The history of money is a process of aestheticization, and the NFT heralds its climax. As money develops from precious metals through bank notes and credit cards to cryptocurrencies, its symbolic nature is incrementally revealed, and its kinship with other forms of symbolic representation becomes clear. The arcane gyrations of financial \u201cderivatives\u201d that constitute today\u2019s economy are entirely figural in nature, and thus ontologically indistinguishable from the manipulation of symbols in art, poetry or music.\n\nIt is this final collapse of aesthetics into economics that dismays the artworld\u2019s commentators, although they do not yet articulate their fears coherently. They are right to be alarmed. The proposed physical destruction of the artwork may (Banksy) or may not (Basquiat) actually happen, but the concept of art as something different from money has already been fatally undermined. Aesthetics and economics are united in the NFT, but theirs will not be a partnership of equals. And while artists and critics may be slow to catch on, economists should easily recognize the merger for what it is: a hostile takeover."
  },
  {
    "id": 11,
    "name": "outliers_book_review.txt",
    "content": "Book Review: Outliers\nby Howard Newell\nIntroduction\nMalcolm Gladwell is a New York Times staff\nwriter who has written several books and articles\nchallenging conventional wisdom. In his book\nOutliers, Malcolm Gladwell (2008) examines\nhow people become successful, and challenges\nthe readers to rethink the stereotype that hard\nwork and brains lead to success. Gladwell uses\nanecdotes, personal interviews, and studies to\nchallenge the notion that success is derived solely\nfrom an individual\u2019s merits, and argues that what\nleads to success is the opportunities, experiences,\nand culture a person has or is exposed to. He\nbreaks the book up into two sections with the first\nsection, \u201cOpportunity,\u201d focusing on opportunities\nand environments that people had in order\nto become successful, and the second section,\n\u201cLegacy,\u201d uncovering how culture can have\nlong-lasting implications for a particular group\u2019s\nsuccess. While Gladwell is able to provide plenty\nof evidence to bolster his argument in the first\nsection regarding a person\u2019s opportunities leading\nto success, he struggles in the second section to\nsuccinctly connect the anecdotes and studies to\nsuccessful outcomes, and oftentimes requires\nthe reader to make a leap of faith to reach the\nconclusion that he provides.\nEvaluation/Analysis\nTIn Gladwell\u2019s first section, \u201cOpportunity,\u201d he\nstates that success is comprised of a combination\nof experience, opportunities, and a certain degree\nof intelligence. But how much experience is\nrequired in order to become successful? Gladwell\nanswers this by pointing to a study conducted by\nK. Anders Ericsson at Berlin\u2019s Academy of Music\n(Ericsson, 2014) that found what separated expert\nmusicians from merely good musicians was that\nthe experts had logged around 10,000 hours of\npractice, whereas the merely good musicians had\nlogged between 4,000 to 6,000 hours. Neurologist\nDaniel Levitin echoes similar sentiments, saying\nthat \u201cthe emerging picture from such studies is\nthat ten thousand hours of practice is required to\nachieve the level of\u2026being a world-class expertin anything\u201d (Gladwell, 2008, p. 40). Gladwell\nfurther illustrates his point by chronicling\nseveral notable people\u2019s rise to success, such as\nBill Gates and The Beatles, and states they had\nthe opportunity and a conducive environment\navailable for them to achieve the critical 10,000-\nhour mark in their respective fields to become\nsuccessful.\nGladwell then segues into his chapter on\nintelligence, arguing that beyond a certain\nthreshold, simply having a high IQ is not a\nguarantee of success. Rather, you simply have to\nbe smart enough in order to be successful. He\nrefers to a study conducted by Richard Lempert at\nthe University of Michigan (Lempert, Chambers,\n& Adams, 2000) who compared minority law\nstudents with lower academic scores to nonminority law students with higher academic\nscores to see if academic ability translated into\nreal-world success. As Lempert discovered, \u201cWe\nfound that they were doing every bit as well. There\nwas no place we saw any serious discrepancy\u201d\n(Gladwell, 2008, p. 85).\nAt the end of the first section, Gladwell has\nprovided a compelling argument with supporting\nevidence to suggest that highly successful people\nare a product of their experience, environment,\nand a certain amount of intelligence, and not\njust their individual personalities or merits. In\nthe next section on legacy, Gladwell opens with\na narrative of a study that provides the reader\nwith the framework that a cultural legacy, even\noriginating from several hundred years ago, can\nshape and impact a person\u2019s success.\nGladwell begins delving into power-distance\nrelationships in the context of airline pilots and\ncrew. Power-distance is typically defined as a\nculture\u2019s acceptance of how power is distributed\namong relationships between leaders and\nsubordinates. Low power-distance cultures are\naccepting of less hierarchical relationships, and\nhigh power-distance cultures typically accept a\nmore hierarchical relationship. Gladwell discusses\na Columbian airline crash that was caused by\nhierarchical command structure between the pilot\nand his aircrew, with the crew uncomfortable\nat explicitly expressing concern over the pilot\u2019s\ndecisions, and writes how Korean Airlines, with\na similar high power-distance culture, was able\nto improve its dismal crash record by retraining\nthe airline crew to adopt a more Western, low\npower-distance environment in the cockpit.\nUnlike the first section\u2019s chapters\u2019 findings that\ncould be applied to a variety of settings, the\nabsence of studies examining power culture or\nother anecdotal evidence from different work\nenvironments is glaring. Gladwell\u2019s polished\nprose and logic in the previous chapters is now\nreduced to an anemic, vague statement of \u201cwhen\nwe understand how much culture and history\nand the world outside of the individual matter\nto professional success\u2026we have a way to make\nsuccesses out of the unsuccessful\u201d (Gladwell, 2008,\np. 220).\nWhat does this really mean? Surely there are\nsuccessful international corporations with\nemployees in different countries who encounter\ncultural differences that could impede success.\nDoes this mean that these corporations have had\nto adopt a low power-distance culture in order\nto achieve success, like the airlines he provides\nas examples? Gladwell simply doesn\u2019t explain\nthe validity of his assertion, leaving the reader\nto assume that his assertion may be only valid\nin airlines or other similar, fast-paced work\nenvironments.\nIn the next chapter, Gladwell postulates the\nstereotype of Asians being good at math could\nhave a basis in their culture of year-round, laborintensive rice farming. He uses proverbs and\nhistorical anecdotes to argue that the concept of\nconstant, hard work has embedded itself into most\nAsian cultures and that this translates into these\ncountries being noticeably better in mathematics,\nbecause they\u2019re more willing to work harder on\nproblems than their Western counterparts. But\nthis is a hard sell; using hand-picked proverbs\nand different agricultural methods as evidence\nthat this is the cause of a country\u2019s success in\nmath seems like an extreme extrapolation, and\nseems jarring compared to the first section\u2019s wellresearched chapters. Instead, the reader is left\nto ponder the absence of studies in this chapter,\nand to question whether Gladwell is providing a\ngross oversimplification of why Asian countries\nperform better in math than Western countries.\nConclusion\nUltimately, Gladwell provides a thoughtful\nexamination of success, and effectively argues\nthat what matters most to becoming successful\nis a person\u2019s experience and opportunities. His\nconcept that culture contributes to success\nunfortunately lacks sufficient evidence and\nappears to be based solely on personal observation\nand opinion. However, the idea itself is intriguing\nand could prompt future research examining\nthis particular relationship to help us better\nunderstand how culture could have an impact in\nobtaining success, and further our knowledge on\nhow to become successful."
  },
  {
    "id": 12,
    "name": "survival_guide_phd.txt",
    "content": "A Survival Guide to a PhD\nSep 7, 2016\n\nThis guide is patterned after my \u201cDoing well in your courses\u201d, a post I wrote a long time ago on some of the tips/tricks I\u2019ve developed during my undergrad. I\u2019ve received nice comments about that guide, so in the same spirit, now that my PhD has come to an end I wanted to compile a similar retrospective document in hopes that it might be helpful to some. Unlike the undergraduate guide, this one was much more difficult to write because there is significantly more variation in how one can traverse the PhD experience. Therefore, many things are likely contentious and a good fraction will be specific to what I\u2019m familiar with (Computer Science / Machine Learning / Computer Vision research). But disclaimers are boring, lets get to it!\n\nPreliminaries\n\nFirst, should you want to get a PhD? I was in a fortunate position of knowing since young age that I really wanted a PhD. Unfortunately it wasn\u2019t for any very well-thought-through considerations: First, I really liked school and learning things and I wanted to learn as much as possible, and second, I really wanted to be like Gordon Freeman from the game Half-Life (who has a PhD from MIT in theoretical physics). I loved that game. But what if you\u2019re more sensible in making your life\u2019s decisions? Should you want to do a PhD? There\u2019s a very nice Quora thread and in the summary of considerations that follows I\u2019ll borrow/restate several from Justin/Ben/others there. I\u2019ll assume that the second option you are considering is joining a medium-large company (which is likely most common). Ask yourself if you find the following properties appealing:\n\nFreedom. A PhD will offer you a lot of freedom in the topics you wish to pursue and learn about. You\u2019re in charge. Of course, you\u2019ll have an adviser who will impose some constraints but in general you\u2019ll have much more freedom than you might find elsewhere.\n\nOwnership. The research you produce will be yours as an individual. Your accomplishments will have your name attached to them. In contrast, it is much more common to \u201cblend in\u201d inside a larger company. A common feeling here is becoming a \u201ccog in a wheel\u201d.\n\nExclusivity. There are very few people who make it to the top PhD programs. You\u2019d be joining a group of a few hundred distinguished individuals in contrast to a few tens of thousands (?) that will join some company.\n\nStatus. Regardless of whether it should be or not, working towards and eventually getting a PhD degree is culturally revered and recognized as an impressive achievement. You also get to be a Doctor; that\u2019s awesome.\n\nPersonal freedom. As a PhD student you\u2019re your own boss. Want to sleep in today? Sure. Want to skip a day and go on a vacation? Sure. All that matters is your final output and no one will force you to clock in from 9am to 5pm. Of course, some advisers might be more or less flexible about it and some companies might be as well, but it\u2019s a true first order statement.\n\nMaximizing future choice. Joining a PhD program doesn\u2019t close any doors or eliminate future employment/lifestyle options. You can go one way (PhD -> anywhere else) but not the other (anywhere else -> PhD -> academia/research; it is statistically less likely). Additionally (although this might be quite specific to applied ML), you\u2019re strictly more hirable as a PhD graduate or even as a PhD dropout and many companies might be willing to put you in a more interesting position or with a higher starting salary. More generally, maximizing choice for the future you is a good heuristic to follow.\n\nMaximizing variance. You\u2019re young and there\u2019s really no need to rush. Once you graduate from a PhD you can spend the next ~50 years of your life in some company. Opt for more variance in your experiences.\n\nPersonal growth. PhD is an intense experience of rapid growth (you learn a lot) and personal self-discovery (you\u2019ll become a master of managing your own psychology). PhD programs (especially if you can make it into a good one) also offer a high density of exceptionally bright people who will become your best friends forever.\n\nExpertise. PhD is probably your only opportunity in life to really drill deep into a topic and become a recognized leading expert in the world at something. You\u2019re exploring the edge of our knowledge as a species, without the burden of lesser distractions or constraints. There\u2019s something beautiful about that and if you disagree, it could be a sign that PhD is not for you.\n\nThe disclaimer. I wanted to also add a few words on some of the potential downsides and failure modes. The PhD is a very specific kind of experience that deserves a large disclaimer. You will inevitably find yourself working very hard (especially before paper deadlines). You need to be okay with the suffering and have enough mental stamina and determination to deal with the pressure. At some points you will lose track of what day of the week it is and go on a diet of leftover food from the microkitchens. You\u2019ll sit exhausted and alone in the lab on a beautiful, sunny Saturday scrolling through Facebook pictures of your friends having fun on exotic trips, paid for by their 5-10x larger salaries. You will have to throw away 3 months of your work while somehow keeping your mental health intact. You\u2019ll struggle with the realization that months of your work were spent on a paper with a few citations while your friends do exciting startups with TechCrunch articles or push products to millions of people. You\u2019ll experience identity crises during which you\u2019ll question your life decisions and wonder what you\u2019re doing with some of the best years of your life. As a result, you should be quite certain that you can thrive in an unstructured environment in the pursuit research and discovery for science. If you\u2019re unsure you should lean slightly negative by default. Ideally you should consider getting a taste of research as an undergraduate on a summer research program before before you decide to commit. In fact, one of the primary reasons that research experience is so desirable during the PhD hiring process is not the research itself, but the fact that the student is more likely to know what they\u2019re getting themselves into.\n\nI should clarify explicitly that this post is not about convincing anyone to do a PhD, I\u2019ve merely tried to enumerate some of the common considerations above. The majority of this post focuses on some tips/tricks for navigating the experience once if you decide to go for it (which we\u2019ll see shortly, below).\n\nLastly, as a random thought I heard it said that you should only do a PhD if you want to go into academia. In light of all of the above I\u2019d argue that a PhD has strong intrinsic value - it\u2019s an end by itself, not just a means to some end (e.g. academic job).\n\nGetting into a PhD program: references, references, references. Great, you\u2019ve decided to go for it. Now how do you get into a good PhD program? The first order approximation is quite simple - by far most important component are strong reference letters. The ideal scenario is that a well-known professor writes you a letter along the lines of: \u201cBlah is in top 5 of students I\u2019ve ever worked with. She takes initiative, comes up with her own ideas, and gets them to work.\u201d The worst letter is along the lines of: \u201cBlah took my class. She did well.\u201d A research publication under your belt from a summer research program is a very strong bonus, but not absolutely required provided you have strong letters. In particular note: grades are quite irrelevant but you generally don\u2019t want them to be too low. This was not obvious to me as an undergrad and I spent a lot of energy on getting good grades. This time should have instead been directed towards research (or at the very least personal projects), as much and as early as possible, and if possible under supervision of multiple people (you\u2019ll need 3+ letters!). As a last point, what won\u2019t help you too much is pestering your potential advisers out of the blue. They are often incredibly busy people and if you try to approach them too aggressively in an effort to impress them somehow in conferences or over email this may agitate them.\n\nPicking the school. Once you get into some PhD programs, how do you pick the school? It\u2019s easy, join Stanford! Just kidding. More seriously, your dream school should 1) be a top school (not because it looks good on your resume/CV but because of feedback loops; top schools attract other top people, many of whom you will get to know and work with) 2) have a few potential advisers you would want to work with. I really do mean the \u201cfew\u201d part - this is very important and provides a safety cushion for you if things don\u2019t work out with your top choice for any one of hundreds of reasons - things in many cases outside of your control, e.g. your dream professor leaves, moves, or spontaneously disappears, and 3) be in a good environment physically. I don\u2019t think new admits appreciate this enough: you will spend 5+ years of your really good years living near the school campus. Trust me, this is a long time and your life will consist of much more than just research.\n\nAdviser\n\nImage credit: PhD comics.\nStudent adviser relationship. The adviser is an extremely important person who will exercise a lot of influence over your PhD experience. It\u2019s important to understand the nature of the relationship: the adviser-student relationship is a symbiosis; you have your own goals and want something out of your PhD, but they also have their own goals, constraints and they\u2019re building their own career. Therefore, it is very helpful to understand your adviser\u2019s incentive structures: how the tenure process works, how they are evaluated, how they get funding, how they fund you, what department politics they might be embedded in, how they win awards, how academia in general works and specifically how they gain recognition and respect of their colleagues. This alone will help you avoid or mitigate a large fraction of student-adviser friction points and allow you to plan appropriately. I also don\u2019t want to make the relationship sound too much like a business transaction. The advisor-student relationship, more often that not, ends up developing into a lasting one, predicated on much more than just career advancement.\n\nPre-vs-post tenure. Every adviser is different so it\u2019s helpful to understand the axes of variations and their repercussions on your PhD experience. As one rule of thumb (and keep in mind there are many exceptions), it\u2019s important to keep track of whether a potential adviser is pre-tenure or post-tenure. The younger faculty members will usually be around more (they are working hard to get tenure) and will usually be more low-level, have stronger opinions on what you should be working on, they\u2019ll do math with you, pitch concrete ideas, or even look at (or contribute to) your code. This is a much more hands-on and possibly intense experience because the adviser will need a strong publication record to get tenure and they are incentivised to push you to work just as hard. In contrast, more senior faculty members may have larger labs and tend to have many other commitments (e.g. committees, talks, travel) other than research, which means that they can only afford to stay on a higher level of abstraction both in the area of their research and in the level of supervision for their students. To caricature, it\u2019s a difference between \u201cyou\u2019re missing a second term in that equation\u201d and \u201cyou may want to read up more in this area, talk to this or that person, and sell your work this or that way\u201d. In the latter case, the low-level advice can still come from the senior PhD students in the lab or the postdocs.\n\nAxes of variation. There are many other axes to be aware of. Some advisers are fluffy and some prefer to keep your relationship very professional. Some will try to exercise a lot of influence on the details of your work and some are much more hands off. Some will have a focus on specific models and their applications to various tasks while some will focus on tasks and more indifference towards any particular modeling approach. In terms of more managerial properties, some will meet you every week (or day!) multiple times and some you won\u2019t see for months. Some advisers answer emails right away and some don\u2019t answer email for a week (or ever, haha). Some advisers make demands about your work schedule (e.g. you better work long hours or weekends) and some won\u2019t. Some advisers generously support their students with equipment and some think laptops or old computers are mostly fine. Some advisers will fund you to go to a conferences even if you don\u2019t have a paper there and some won\u2019t. Some advisers are entrepreneurial or applied and some lean more towards theoretical work. Some will let you do summer internships and some will consider internships just a distraction.\n\nFinding an adviser. So how do you pick an adviser? The first stop, of course, is to talk to them in person. The student-adviser relationship is sometimes referred to as a marriage and you should make sure that there is a good fit. Of course, first you want to make sure that you can talk with them and that you get along personally, but it\u2019s also important to get an idea of what area of \u201cprofessor space\u201d they occupy with respect to the aforementioned axes, and especially whether there is an intellectual resonance between the two of you in terms of the problems you are interested in. This can be just as important as their management style.\n\nCollecting references. You should also collect references on your potential adviser. One good strategy is to talk to their students. If you want to get actual information this shouldn\u2019t be done in a very formal way or setting but in a relaxed environment or mood (e.g. a party). In many cases the students might still avoid saying bad things about the adviser if asked in a general manner, but they will usually answer truthfully when you ask specific questions, e.g. \u201chow often do you meet?\u201d, or \u201chow hands on are they?\u201d. Another strategy is to look at where their previous students ended up (you can usually find this on the website under an alumni section), which of course also statistically informs your own eventual outcome.\n\nImpressing an adviser. The adviser-student matching process is sometimes compared to a marriage - you pick them but they also pick you. The ideal student from their perspective is someone with interest and passion, someone who doesn\u2019t need too much hand-holding, and someone who takes initiative - who shows up a week later having done not just what the adviser suggested, but who went beyond it; improved on it in unexpected ways.\n\nConsider the entire lab. Another important point to realize is that you\u2019ll be seeing your adviser maybe once a week but you\u2019ll be seeing most of their students every single day in the lab and they will go on to become your closest friends. In most cases you will also end up collaborating with some of the senior PhD students or postdocs and they will play a role very similar to that of your adviser. The postdocs, in particular, are professors-in-training and they will likely be eager to work with you as they are trying to gain advising experience they can point to for their academic job search. Therefore, you want to make sure the entire group has people you can get along with, people you respect and who you can work with closely on research projects.\n\nResearch topics\n\nt-SNE visualization of a small subset of human knowledge (from paperscape). Each circle is an arxiv paper and size indicates the number of citations.\nSo you\u2019ve entered a PhD program and found an adviser. Now what do you work on?\n\nAn exercise in the outer loop. First note the nature of the experience. A PhD is simultaneously a fun and frustrating experience because you\u2019re constantly operating on a meta problem level. You\u2019re not just solving problems - that\u2019s merely the simple inner loop. You spend most of your time on the outer loop, figuring out what problems are worth solving and what problems are ripe for solving. You\u2019re constantly imagining yourself solving hypothetical problems and asking yourself where that puts you, what it could unlock, or if anyone cares. If you\u2019re like me this can sometimes drive you a little crazy because you\u2019re spending long hours working on things and you\u2019re not even sure if they are the correct things to work on or if a solution exists.\n\nDeveloping taste. When it comes to choosing problems you\u2019ll hear academics talk about a mystical sense of \u201ctaste\u201d. It\u2019s a real thing. When you pitch a potential problem to your adviser you\u2019ll either see their face contort, their eyes rolling, and their attention drift, or you\u2019ll sense the excitement in their eyes as they contemplate the uncharted territory ripe for exploration. In that split second a lot happens: an evaluation of the problem\u2019s importance, difficulty, its sexiness, its historical context (and possibly also its fit to their active grants). In other words, your adviser is likely to be a master of the outer loop and will have a highly developed sense of taste for problems. During your PhD you\u2019ll get to acquire this sense yourself.\n\nIn particular, I think I had a terrible taste coming in to the PhD. I can see this from the notes I took in my early PhD years. A lot of the problems I was excited about at the time were in retrospect poorly conceived, intractable, or irrelevant. I\u2019d like to think I refined the sense by the end through practice and apprenticeship.\n\nLet me now try to serialize a few thoughts on what goes into this sense of taste, and what makes a problem interesting to work on.\n\nA fertile ground. First, recognize that during your PhD you will dive deeply into one area and your papers will very likely chain on top of each other to create a body of work (which becomes your thesis). Therefore, you should always be thinking several steps ahead when choosing a problem. It\u2019s impossible to predict how things will unfold but you can often get a sense of how much room there could be for additional work.\n\nPlays to your adviser\u2019s interests and strengths. You will want to operate in the realm of your adviser\u2019s interest. Some advisers may allow you to work on slightly tangential areas but you would not be taking full advantage of their knowledge and you are making them less likely to want to help you with your project or promote your work. For instance, (and this goes to my previous point of understanding your adviser\u2019s job) every adviser has a \u201cdefault talk\u201d slide deck on their research that they give all the time and if your work can add new exciting cutting edge work slides to this deck then you\u2019ll find them much more invested, helpful and involved in your research. Additionally, their talks will promote and publicize your work.\n\nBe ambitious: the sublinear scaling of hardness. People have a strange bug built into psychology: a 10x more important or impactful problem intuitively feels 10x harder (or 10x less likely) to achieve. This is a fallacy - in my experience a 10x more important problem is at most 2-3x harder to achieve. In fact, in some cases a 10x harder problem may be easier to achieve. How is this? It\u2019s because thinking 10x forces you out of the box, to confront the real limitations of an approach, to think from first principles, to change the strategy completely, to innovate. If you aspire to improve something by 10% and work hard then you will. But if you aspire to improve it by 100% you are still quite likely to, but you will do it very differently.\n\nAmbitious but with an attack. At this point it\u2019s also important to point out that there are plenty of important problems that don\u2019t make great projects. I recommend reading You and Your Research by Richard Hamming, where this point is expanded on:\n\nIf you do not work on an important problem, it\u2019s unlikely you\u2019ll do important work. It\u2019s perfectly obvious. Great scientists have thought through, in a careful way, a number of important problems in their field, and they keep an eye on wondering how to attack them. Let me warn you, `important problem\u2019 must be phrased carefully. The three outstanding problems in physics, in a certain sense, were never worked on while I was at Bell Labs. By important I mean guaranteed a Nobel Prize and any sum of money you want to mention. We didn\u2019t work on (1) time travel, (2) teleportation, and (3) antigravity. They are not important problems because we do not have an attack. It\u2019s not the consequence that makes a problem important, it is that you have a reasonable attack. That is what makes a problem important.\n\nThe person who did X. Ultimately, the goal of a PhD is to not only develop a deep expertise in a field but to also make your mark upon it. To steer it, shape it. The ideal scenario is that by the end of the PhD you own some part of an important area, preferably one that is also easy and fast to describe. You want people to say things like \u201cshe\u2019s the person who did X\u201d. If you can fill in a blank there you\u2019ll be successful.\n\nValuable skills. Recognize that during your PhD you will become an expert at the area of your choosing (as fun aside, note that [5 years]x[260 working days]x[8 hours per day] is 10,400 hours; if you believe Gladwell then a PhD is exactly the amount of time to become an expert). So imagine yourself 5 years later being a world expert in this area (the 10,000 hours will ensure that regardless of the academic impact of your work). Are these skills exciting or potentially valuable to your future endeavors?\n\nNegative examples. There are also some problems or types of papers that you ideally want to avoid. For instance, you\u2019ll sometimes hear academics talk about \u201cincremental work\u201d (this is the worst adjective possible in academia). Incremental work is a paper that enhances something existing by making it more complex and gets 2% extra on some benchmark. The amusing thing about these papers is that they have a reasonably high chance of getting accepted (a reviewer can\u2019t point to anything to kill them; they are also sometimes referred to as \u201ccockroach papers\u201d), so if you have a string of these papers accepted you can feel as though you\u2019re being very productive, but in fact these papers won\u2019t go on to be highly cited and you won\u2019t go on to have a lot of impact on the field. Similarly, finding projects should ideally not include thoughts along the lines of \u201cthere\u2019s this next logical step in the air that no one has done yet, let me do it\u201d, or \u201cthis should be an easy poster\u201d.\n\nCase study: my thesis. To make some of this discussion more concrete I wanted to use the example of how my own PhD unfolded. First, fun fact: my entire thesis is based on work I did in the last 1.5 years of my PhD. i.e. it took me quite a long time to wiggle around in the metaproblem space and find a problem that I felt very excited to work on (the other ~2 years I mostly meandered on 3D things (e.g. Kinect Fusion, 3D meshes, point cloud features) and video things). Then at one point in my 3rd year I randomly stopped by Richard Socher\u2019s office on some Saturday at 2am. We had a chat about interesting problems and I realized that some of his work on images and language was in fact getting at something very interesting (of course, the area at the intersection of images and language goes back quite a lot further than Richard as well). I couldn\u2019t quite see all the papers that would follow but it seemed heuristically very promising: it was highly fertile (a lot of unsolved problems, a lot of interesting possibilities on grounding descriptions to images), I felt that it was very cool and important, it was easy to explain, it seemed to be at the boundary of possible (Deep Learning has just started to work), the datasets had just started to become available (Flickr8K had just come out), it fit nicely into Fei-Fei\u2019s interests and even if I were not successful I\u2019d at least get lots of practice with optimizing interesting deep nets that I could reapply elsewhere. I had a strong feeling of a tsunami of checkmarks as everything clicked in place in my mind. I pitched this to Fei-Fei (my adviser) as an area to dive into the next day and, with relief, she enthusiastically approved, encouraged me, and would later go on to steer me within the space (e.g. Fei-Fei insisted that I do image to sentence generation while I was mostly content with ranking.). I\u2019m happy with how things evolved from there. In short, I meandered around for 2 years stuck around the outer loop, finding something to dive into. Once it clicked for me what that was based on several heuristics, I dug in.\n\nResistance. I\u2019d like to also mention that your adviser is by no means infallible. I\u2019ve witnessed and heard of many instances in which, in retrospect, the adviser made the wrong call. If you feel this way during your phd you should have the courage to sometimes ignore your adviser. Academia generally celebrates independent thinking but the response of your specific adviser can vary depending on circumstances. I\u2019m aware of multiple cases where the bet worked out very well and I\u2019ve also personally experienced cases where it did not. For instance, I disagreed strongly with some advice Andrew Ng gave me in my very first year. I ended up working on a problem he wasn\u2019t very excited about and, surprise, he turned out to be very right and I wasted a few months. Win some lose some :)\n\nDon\u2019t play the game. Finally, I\u2019d like to challenge you to think of a PhD as more than just a sequence of papers. You\u2019re not a paper writer. You\u2019re a member of a research community and your goal is to push the field forward. Papers are one common way of doing that but I would encourage you to look beyond the established academic game. Think for yourself and from first principles. Do things others don\u2019t do but should. Step off the treadmill that has been put before you. I tried to do some of this myself throughout my PhD. This blog is an example - it allows me communicate things that wouldn\u2019t ordinarily go into papers. The ImageNet human reference experiments are an example - I felt strongly that it was important for the field to know the ballpark human accuracy on ILSVRC so I took a few weeks off and evaluated it. The academic search tools (e.g. arxiv-sanity) are an example - I felt continuously frustrated by the inefficiency of finding papers in the literature and I released and maintain the site in hopes that it can be useful to others. Teaching CS231n twice is an example - I put much more effort into it than is rationally advisable for a PhD student who should be doing research, but I felt that the field was held back if people couldn\u2019t efficiently learn about the topic and enter. A lot of my PhD endeavors have likely come at a cost in standard academic metrics (e.g. h-index, or number of publications in top venues) but I did them anyway, I would do it the same way again, and here I am encouraging others to as well. To add a pitch of salt and wash down the ideology a bit, based on several past discussions with my friends and colleagues I know that this view is contentious and that many would disagree.\n\nWriting papers\n\nWriting good papers is an essential survival skill of an academic (kind of like making fire for a caveman). In particular, it is very important to realize that papers are a specific thing: they look a certain way, they flow a certain way, they have a certain structure, language, and statistics that the other academics expect. It\u2019s usually a painful exercise for me to look through some of my early PhD paper drafts because they are quite terrible. There is a lot to learn here.\n\nReview papers. If you\u2019re trying to learn to write better papers it can feel like a sensible strategy to look at many good papers and try to distill patterns. This turns out to not be the best strategy; it\u2019s analogous to only receiving positive examples for a binary classification problem. What you really want is to also have exposure to a large number of bad papers and one way to get this is by reviewing papers. Most good conferences have an acceptance rate of about 25% so most papers you\u2019ll review are bad, which will allow you to build a powerful binary classifier. You\u2019ll read through a bad paper and realize how unclear it is, or how it doesn\u2019t define it\u2019s variables, how vague and abstract its intro is, or how it dives in to the details too quickly, and you\u2019ll learn to avoid the same pitfalls in your own papers. Another related valuable experience is to attend (or form) journal clubs - you\u2019ll see experienced researchers critique papers and get an impression for how your own papers will be analyzed by others.\n\nGet the gestalt right. I remember being impressed with Fei-Fei (my adviser) once during a reviewing session. I had a stack of 4 papers I had reviewed over the last several hours and she picked them up, flipped through each one for 10 seconds, and said one of them was good and the other three bad. Indeed, I was accepting the one and rejecting the other three, but something that took me several hours took her seconds. Fei-Fei was relying on the gestalt of the papers as a powerful heuristic. Your papers, as you become a more senior researcher take on a characteristic look. An introduction of ~1 page. A ~1 page related work section with a good density of citations - not too sparse but not too crowded. A well-designed pull figure (on page 1 or 2) and system figure (on page 3) that were not made in MS Paint. A technical section with some math symbols somewhere, results tables with lots of numbers and some of them bold, one additional cute analysis experiment, and the paper has exactly 8 pages (the page limit) and not a single line less. You\u2019ll have to learn how to endow your papers with the same gestalt because many researchers rely on it as a cognitive shortcut when they judge your work.\n\nIdentify the core contribution. Before you start writing anything it\u2019s important to identify the single core contribution that your paper makes to the field. I would especially highlight the word single. A paper is not a random collection of some experiments you ran that you report on. The paper sells a single thing that was not obvious or present before. You have to argue that the thing is important, that it hasn\u2019t been done before, and then you support its merit experimentally in controlled experiments. The entire paper is organized around this core contribution with surgical precision. In particular it doesn\u2019t have any additional fluff and it doesn\u2019t try to pack anything else on a side. As a concrete example, I made a mistake in one of my earlier papers on video classification where I tried to pack in two contributions: 1) a set of architectural layouts for video convnets and an unrelated 2) multi-resolution architecture which gave small improvements. I added it because I reasoned first that maybe someone could find it interesting and follow up on it later and second because I thought that contributions in a paper are additive: two contributions are better than one. Unfortunately, this is false and very wrong. The second contribution was minor/dubious and it diluted the paper, it was distracting, and no one cared. I\u2019ve made a similar mistake again in my CVPR 2014 paper which presented two separate models: a ranking model and a generation model. Several good in-retrospect arguments could be made that I should have submitted two separate papers; the reason it was one is more historical than rational.\n\nThe structure. Once you\u2019ve identified your core contribution there is a default recipe for writing a paper about it. The upper level structure is by default Intro, Related Work, Model, Experiments, Conclusions. When I write my intro I find that it helps to put down a coherent top-level narrative in latex comments and then fill in the text below. I like to organize each of my paragraphs around a single concrete point stated on the first sentence that is then supported in the rest of the paragraph. This structure makes it easy for a reader to skim the paper. A good flow of ideas is then along the lines of 1) X (+define X if not obvious) is an important problem 2) The core challenges are this and that. 2) Previous work on X has addressed these with Y, but the problems with this are Z. 3) In this work we do W (?). 4) This has the following appealing properties and our experiments show this and that. You can play with this structure a bit but these core points should be clearly made. Note again that the paper is surgically organized around your exact contribution. For example, when you list the challenges you want to list exactly the things that you address later; you don\u2019t go meandering about unrelated things to what you have done (you can speculate a bit more later in conclusion). It is important to keep a sensible structure throughout your paper, not just in the intro. For example, when you explain the model each section should: 1) explain clearly what is being done in the section, 2) explain what the core challenges are 3) explain what a baseline approach is or what others have done before 4) motivate and explain what you do 5) describe it.\n\nBreak the structure. You should also feel free (and you\u2019re encouraged to!) play with these formulas to some extent and add some spice to your papers. For example, see this amusing paper from Razavian et al. in 2014 that structures the introduction as a dialog between a student and the professor. It\u2019s clever and I like it. As another example, a lot of papers from Alyosha Efros have a playful tone and make great case studies in writing fun papers. As only one of many examples, see this paper he wrote with Antonio Torralba: Unbiased look at dataset bias. Another possibility I\u2019ve seen work well is to include an FAQ section, possibly in the appendix.\n\nCommon mistake: the laundry list. One very common mistake to avoid is the \u201claundry list\u201d, which looks as follows: \u201cHere is the problem. Okay now to solve this problem first we do X, then we do Y, then we do Z, and now we do W, and here is what we get\u201d. You should try very hard to avoid this structure. Each point should be justified, motivated, explained. Why do you do X or Y? What are the alternatives? What have others done? It\u2019s okay to say things like this is common (add citation if possible). Your paper is not a report, an enumeration of what you\u2019ve done, or some kind of a translation of your chronological notes and experiments into latex. It is a highly processed and very focused discussion of a problem, your approach and its context. It is supposed to teach your colleagues something and you have to justify your steps, not just describe what you did.\n\nThe language. Over time you\u2019ll develop a vocabulary of good words and bad words to use when writing papers. Speaking about machine learning or computer vision papers specifically as concrete examples, in your papers you never \u201cstudy\u201d or \u201cinvestigate\u201d (there are boring, passive, bad words); instead you \u201cdevelop\u201d or even better you \u201cpropose\u201d. And you don\u2019t present a \u201csystem\u201d or, shudder, a \u201cpipeline\u201d; instead, you develop a \u201cmodel\u201d. You don\u2019t learn \u201cfeatures\u201d, you learn \u201crepresentations\u201d. And god forbid, you never \u201ccombine\u201d, \u201cmodify\u201d or \u201cexpand\u201d. These are incremental, gross terms that will certainly get your paper rejected :).\n\nAn internal deadlines 2 weeks prior. Not many labs do this, but luckily Fei-Fei is quite adamant about an internal deadline 2 weeks before the due date in which you must submit at least a 5-page draft with all the final experiments (even if not with final numbers) that goes through an internal review process identical to the external one (with the same review forms filled out, etc). I found this practice to be extremely useful because forcing yourself to lay out the full paper almost always reveals some number of critical experiments you must run for the paper to flow and for its argument flow to be coherent, consistent and convincing.\n\nAnother great resource on this topic is Tips for Writing Technical Papers from Jennifer Widom.\n\nWriting code\n\nA lot of your time will of course be taken up with the execution of your ideas, which likely involves a lot of coding. I won\u2019t dwell on this too much because it\u2019s not uniquely academic, but I would like to bring up a few points.\n\nRelease your code. It\u2019s a somewhat surprising fact but you can get away with publishing papers and not releasing your code. You will also feel a lot of incentive to not release your code: it can be a lot of work (research code can look like spaghetti since you iterate very quickly, you have to clean up a lot), it can be intimidating to think that others might judge you on your at most decent coding abilities, it is painful to maintain code and answer questions from other people about it (forever), and you might also be concerned that people could spot bugs that invalidate your results. However, it is precisely for some of these reasons that you should commit to releasing your code: it will force you to adopt better coding habits due to fear of public shaming (which will end up saving you time!), it will force you to learn better engineering practices, it will force you to be more thorough with your code (e.g. writing unit tests to make bugs much less likely), it will make others much more likely to follow up on your work (and hence lead to more citations of your papers) and of course it will be much more useful to everyone as a record of exactly what was done for posterity. When you do release your code I recommend taking advantage of docker containers; this will reduce the amount of headaches people email you about when they can\u2019t get all the dependencies (and their precise versions) installed.\n\nThink of the future you. Make sure to document all your code very well for yourself. I guarantee you that you will come back to your code base a few months later (e.g. to do a few more experiments for the camera ready version of the paper), and you will feel completely lost in it. I got into the habit of creating very thorough readme.txt files in all my repos (for my personal use) as notes to future self on how the code works, how to run it, etc.\n\nGiving talks\n\nSo, you published a paper and it\u2019s an oral! Now you get to give a few minute talk to a large audience of people - what should it look like?\n\nThe goal of a talk. First, that there\u2019s a common misconception that the goal of your talk is to tell your audience about what you did in your paper. This is incorrect, and should only be a second or third degree design criterion. The goal of your talk is to 1) get the audience really excited about the problem you worked on (they must appreciate it or they will not care about your solution otherwise!) 2) teach the audience something (ideally while giving them a taste of your insight/solution; don\u2019t be afraid to spend time on other\u2019s related work), and 3) entertain (they will start checking their Facebook otherwise). Ideally, by the end of the talk the people in your audience are thinking some mixture of \u201cwow, I\u2019m working in the wrong area\u201d, \u201cI have to read this paper\u201d, and \u201cThis person has an impressive understanding of the whole area\u201d.\n\nA few do\u2019s: There are several properties that make talks better. For instance, Do: Lots of pictures. People Love pictures. Videos and animations should be used more sparingly because they distract. Do: make the talk actionable - talk about something someone can do after your talk. Do: give a live demo if possible, it can make your talk more memorable. Do: develop a broader intellectual arch that your work is part of. Do: develop it into a story (people love stories). Do: cite, cite, cite - a lot! It takes very little slide space to pay credit to your colleagues. It pleases them and always reflects well on you because it shows that you\u2019re humble about your own contribution, and aware that it builds on a lot of what has come before and what is happening in parallel. You can even cite related work published at the same conference and briefly advertise it. Do: practice the talk! First for yourself in isolation and later to your lab/friends. This almost always reveals very insightful flaws in your narrative and flow.\n\nDon\u2019t: texttexttext. Don\u2019t crowd your slides with text. There should be very few or no bullet points - speakers sometimes try to use these as a crutch to remind themselves what they should be talking about but the slides are not for you they are for the audience. These should be in your speaker notes. On the topic of crowding the slides, also avoid complex diagrams as much as you can - your audience has a fixed bit bandwidth and I guarantee that your own very familiar and \u201csimple\u201d diagram is not as simple or interpretable to someone seeing it for the first time.\n\nCareful with: result tables: Don\u2019t include dense tables of results showing that your method works better. You got a paper, I\u2019m sure your results were decent. I always find these parts boring and unnecessary unless the numbers show something interesting (other than your method works better), or of course unless there is a large gap that you\u2019re very proud of. If you do include results or graphs build them up slowly with transitions, don\u2019t post them all at once and spend 3 minutes on one slide.\n\nPitfall: the thin band between bored/confused. It\u2019s actually quite tricky to design talks where a good portion of your audience learns something. A common failure case (as an audience member) is to see talks where I\u2019m painfully bored during the first half and completely confused during the second half, learning nothing by the end. This can occur in talks that have a very general (too general) overview followed by a technical (too technical) second portion. Try to identify when your talk is in danger of having this property.\n\nPitfall: running out of time. Many speakers spend too much time on the early intro parts (that can often be somewhat boring) and then frantically speed through all the last few slides that contain the most interesting results, analysis or demos. Don\u2019t be that person.\n\nPitfall: formulaic talks. I might be a special case but I\u2019m always a fan of non-formulaic talks that challenge conventions. For instance, I despise the outline slide. It makes the talk so boring, it\u2019s like saying: \u201cThis movie is about a ring of power. In the first chapter we\u2019ll see a hobbit come into possession of the ring. In the second we\u2019ll see him travel to Mordor. In the third he\u2019ll cast the ring into Mount Doom and destroy it. I will start with chapter 1\u201d - Come on! I use outline slides for much longer talks to keep the audience anchored if they zone out (at 30min+ they inevitably will a few times), but it should be used sparingly.\n\nObserve and learn. Ultimately, the best way to become better at giving talks (as it is with writing papers too) is to make conscious effort to pay attention to what great (and not so great) speakers do and build a binary classifier in your mind. Don\u2019t just enjoy talks; analyze them, break them down, learn from them. Additionally, pay close attention to the audience and their reactions. Sometimes a speaker will put up a complex table with many numbers and you will notice half of the audience immediately look down on their phone and open Facebook. Build an internal classifier of the events that cause this to happen and avoid them in your talks.\n\nAttending conferences\n\nOn the subject of conferences:\n\nGo. It\u2019s very important that you go to conferences, especially the 1-2 top conferences in your area. If your adviser lacks funds and does not want to pay for your travel expenses (e.g. if you don\u2019t have a paper) then you should be willing to pay for yourself (usually about $2000 for travel, accommodation, registration and food). This is important because you want to become part of the academic community and get a chance to meet more people in the area and gossip about research topics. Science might have this image of a few brilliant lone wolfs working in isolation, but the truth is that research is predominantly a highly social endeavor - you stand on the shoulders of many people, you\u2019re working on problems in parallel with other people, and it is these people that you\u2019re also writing papers to. Additionally, it\u2019s unfortunate but each field has knowledge that doesn\u2019t get serialized into papers but is instead spread across a shared understanding of the community; things such as what are the next important topics to work on, what papers are most interesting, what is the inside scoop on papers, how they developed historically, what methods work (not just on paper, in reality), etcetc. It is very valuable (and fun!) to become part of the community and get direct access to the hivemind - to learn from it first, and to hopefully influence it later.\n\nTalks: choose by speaker. One conference trick I\u2019ve developed is that if you\u2019re choosing which talks to attend it can be better to look at the speakers instead of the topics. Some people give better talks than others (it\u2019s a skill, and you\u2019ll discover these people in time) and in my experience I find that it often pays off to see them speak even if it is on a topic that isn\u2019t exactly connected to your area of research.\n\nThe real action is in the hallways. The speed of innovation (especially in Machine Learning) now works at timescales much faster than conferences so most of the relevant papers you\u2019ll see at the conference are in fact old news. Therefore, conferences are primarily a social event. Instead of attending a talk I encourage you to view the hallway as one of the main events that doesn\u2019t appear on the schedule. It can also be valuable to stroll the poster session and discover some interesting papers and ideas that you may have missed.\n\nIt is said that there are three stages to a PhD. In the first stage you look at a related paper\u2019s reference section and you haven\u2019t read most of the papers. In the second stage you recognize all the papers. In the third stage you\u2019ve shared a beer with all the first authors of all the papers.\n\nClosing thoughts\nI can\u2019t find the quote anymore but I heard Sam Altman of YC say that there are no shortcuts or cheats when it comes to building a startup. You can\u2019t expect to win in the long run by somehow gaming the system or putting up false appearances. I think that the same applies in academia. Ultimately you\u2019re trying to do good research and push the field forward and if you try to game any of the proxy metrics you won\u2019t be successful in the long run. This is especially so because academia is in fact surprisingly small and highly interconnected, so anything shady you try to do to pad your academic resume (e.g. self-citing a lot, publishing the same idea multiple times with small remixes, resubmitting the same rejected paper over and over again with no changes, conveniently trying to leave out some baselines etc.) will eventually catch up with you and you will not be successful.\n\nSo at the end of the day it\u2019s quite simple. Do good work, communicate it properly, people will notice and good things will happen. Have a fun ride!"
  },
  {
    "id": 13,
    "name": "which_learning_methods_work.txt",
    "content": "Which Learning Methods Actually Work?\n\nHere\u2019s an interesting article on the effectiveness of various study techniques\u2014and in particular\u2014which ones have evidence supporting them.\n\nSome of my thoughts on the key findings:\n\nSelf-Explanation and Reading\nElaborative learning and self-explanation were found to be moderately effective. This is similar to the Feynman technique, but I\u2019d argue the use of the method was different (I mainly use this method to hunt out specific misunderstandings, not as a general catch-all which is usually too time consuming as indicated by the research).\n\nSummarizing and highlighting were found to be ineffective. I was surprised about the finding on summarizing, but the note on highlighting was what I expected. Perhaps some of the problem with summarizing is that it lacks a feedback mechanism to know whether you\u2019ve actually gathered the key details?\n\nRereading was found to be ineffective. No surprises here. Passive review strategies are less effective than active ones.\n\nVisualization and Mnemonics\nThe keyword mnemonic (using visual links to memorize words) was labelled \u201cineffective\u201d but probably a better description of the actual findings is that it has more narrow usage.\n\nVisualization while reading was found to be ineffective. I found this interesting, given my advice to students to visualize. However, it seems like the issue may be that visualization while reading is distracting. In addition, I\u2019ve always felt the major benefits of visualizing are for abstract subjects, which don\u2019t naturally lend themselves to images, as opposed to the concrete subjects measured here.\n\nPractice and Spacing Were The Most Effective Methods\nPractice was one of the most effective methods studied. This was a staple of the MIT Challenge, probably making up 50% of the total time I spend working through the courses. I found it interesting that practice questions were still effective even when you created the questions yourself\u2014a good alternative if practice exams are unavailable.\n\nSpacing effects were shown as being effective as well. During the MIT Challenge I tried to make use of spacing as much as possible, doing classes in parallel after the first few classes. It was an unfortunate weakness of the tight time-constraint premise of the challenge, however. I\u2019d recommend most students looking to follow my self-education attempt to spread out their learning over a longer period of time and at a lower intensity than I did.\n\nI find the spacing effects on learning to be somewhat tricky because I\u2019ve also found focus to be enormously effective for getting things done. Juggling a couple different learning projects at the same time to be far harder to manage than being dedicated to only one or two.\n\nUltimately, there may be a trade-off between spacing and focus\u2014you want the spacing to ensure better long-term recall, but you want the focus to actually get the work done.\n\nScience, Experience and What Actually Works\nScientific research on the efficacy of different learning techniques has been a very useful counterbalance for me to the methods I\u2019ve developed through practical experience.\n\nI\u2019m only a single data point. Although I collect a lot of observations and data from my students in my courses, the empirical rigor of a self-selected online course is not the same as a scientific paper. Placebo effects and the lack of control groups mean that it\u2019s important to return to the psychological research for checks and balances.\n\nHowever studies are often designed to measure the outcome of a very narrow set of conditions. If the conditions change from the experiment, the results may be very different. I end up relying a lot on practical experience to fill in those gaps in my own self-learning efforts.\n\nOpinions I\u2019ve Changed Since Learn More, Study Less\nI find myself learning a lot about learning, as more research like this comes out and I get exposed to different learning situations and am forced to adapt. While I still support the most of the main points of Learn More, Study Less, I\u2019ve evolved considerably on my views since then.\n\nHere are the major opinions I\u2019ve shifted:\n\nRepetition isn\u2019t a bad thing (even if repetition alone probably is).\nSpeed reading is only narrowly useful. In most learning situations, reading at a deeper level of processing and reading more slowly is better.\nPractice and active recall should be a bulk of your strategy. I haven\u2019t given enough emphasis on active recall in the past, even though it should probably form a large chunk of your learning time.\nSpaced repetition software can be quite useful. I\u2019ve flipped my thinking on this point since I mentioned it earlier. To me the disadvantages of decontextualized and unprioritized knowledge are outweighed by the automatic structuring of review and active recall.\nDon\u2019t highlight. I used to have highlighting as part of an active reading strategy, but now I\u2019m inclined to avoid it altogether. Taking sparse notes is better.\nHolistic learning is still valid for law and languages. I expressed doubts in my initial ebook as to whether learning via connections was appropriate for densely factual subjects, but since then I\u2019ve found it useful for these subjects nonetheless.\nA major challenge for me is that, in spending a lot of time learning, my opinions grow with time. Hopefully my minor reversals and shifts in emphasis don\u2019t irk or confuse longtime readers too much. The alternative is to be dogmatic, an unsupportable long-term strategy."
  }
]